# 쿠버네티스 내부 이해

## 아키텍처 이해

컨트롤 플레인 구성 요소

* etcd 분산 저장 스토리지
* API 서버
* 스케줄러
* 컨트롤러 매니저

워커 노드에서 실행하는 구성 요소

* Kubelet
* 쿠버네티스 서비스 프록시
* 컨테이너 런타임

애드온 구성 요소

* 쿠버네티스 DNS 서버
* 대시보드
* 인그레스 컨트롤러
* 힙스터
* 컨테이너 네트워크 인터페이스 플러그인

<img width="509" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/50b3a53a-1dac-4738-b804-28cc060cbed1">

```shell
k get componentstatuses

NAME                 STATUS    MESSAGE                         ERROR
controller-manager   Healthy   ok                              
scheduler            Healthy   ok                              
etcd-0               Healthy   {"health":"true","reason":""}   
```

구성요소들은 오직 API 서버하고만 통신한다. API 서버는 etcd와 통신하는 유일한 구성 요소다. 다른 구성요소는 etcd와 직접 통신하지 않고, API 서버로 클러스터 상태를 변경한다.  

위의 그림과 같이 API 서버와 구성 요소 사이의 통신은 대부분 구성 요소에서 시작하지만, kubectl을 이용해 로그를 가져오거나 kubectl attach 명령으로 실행 중인 컨테이너에 연결할 때 kubectl port-forward 명령을 실행할 때는 API 서버가 Kubelet에 접속한다.  

컨트롤 플레인을 둘 이상 두어 병렬로 수행할 수 있다. 이때 etcd와 API 서버는 여러 인스턴스를 동시에 활성화해 작업을 병렬로 수행할 수 있지만, 스케줄러와 컨트롤러 매니저는 하나의 인스턴스만 활성화되고 나머지는 대기 상태에 있게 된다.  

Kubelet은 유일하게 일반 시스템 구성 요소로 실행되며, Kubelet이 다른 구성 요소를 파드에 실행한다. 컨트롤 플레인 구성 요소를 파드로 실행하기 위해 Kubelet도 마스터 노드에 배포된다.  

<Br/>

### 쿠버네티스가 etcd를 사용하는 방법

모든 오브젝트는 API 서버가 다시 시작되거나 실패하더라도 유지하기 위해 매니페스트가 영구적으로 저장될 필요가 있다. 이를 위해 쿠버네티스는 빠르고, 분산해서 저장되며, 일관된 키-값 저장소를 제공하는 etcd를 사용한다. 분산돼 있기 때문에 둘 이상의 etcd 인스턴스를 실행해 고가용성과 우수한 성능을 제공할 수 있다.  

#### 저장된 오브젝트의 일관성과 유효성 보장

쿠버네티스는 etcd 접근을 API 서버로만 하고 다른 구성 요소가 API서버를 통하게 함으로써 API 서버 한곳에서 낙관적 잠금 메커니즘을 구현해 구성 요소의 데이터 불일치를 개선시켰다. 또한 API 서버는 저장소에 기록된 데이터가 항상 유효하고 데이터의 변경이 올바른 권한을 가진 클라이언트에 의해서만 수행되도록 한다.  

#### 클러스터링된 etcd의 일관성 보장

두 개 이상의 etcd 인스턴스를 사용하면서 etcd는 RAFT 합의 알고리즘을 사용해 어느 순간이든 각 노드 상태가 대다수의 노드가 동의하는 현재 상태이거나 이전에 동의된 상태 중에 하나임을 보장한다.  

합의 알고리즘은 클러스터가 다음 상태로 진행하기 위해 과반수가 필요하다. etcd 인스턴스 수가 홀수인 이유이기도 하다.

<img width="766" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/78f41744-a762-4ee1-b415-1ef08dd447bb">

<br/>

### API 서버의 기능

다른 구성요소로부터 클러스터 상태를 조회하고 변경하기 위해 RESTFul API로 CRUD 인터페이스를 제공하고 상태는 etcd안에 저장한다.  

오브젝트를 etcd에 저장하는 일관된 방법을 제공하는 것뿐만 아니라, 오브젝트 유효성 검사 작업도 수행하기 때문에 잘못 설정된 오브젝트를 저장할 수 없다.  

낙관적 잠금도 처리하기 때문에 동시에 업데이터가 발생하더라도 다른 클라이언트에 의해 오브젝트의 변경 사항이 재정의되지 않는다.  

<img width="851" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/cbd4b141-b9c2-4eae-ac4d-1f68248a69f1">

API 서버에 구성된 하나 이상의 플러그인을 이용해 요청을 보낸 클라이언트를 인증한다. 인가 또한 마찬가지다.  
리소스를 생성, 수정, 삭제하려는 요청인 경우에 해당 요청은 어드미션 컨트롤로 보내진다.  
요청이 모든 어드미션 컨트롤 플러그인을 통과하면, API 서버는 오브젝트의 유효성을 검증하고 etcd에 저장한다.  

API 서버는 레플리카셋 리소스를 만들 때 파드를 만들지 않고 서비스의 엔드포인트를 관리하지 않는다. 이것들은 컨트롤러 매니저의 컨트롤러들이 하는 일이다. API 서버는 컨트롤러한테 무엇을 해야하는지 알려주지 않고 컨트롤러와 다른 구성요소가 배포된 리소스의 변경 사항을 관찰할 수 있게끔 한다.  

<img width="851" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/3a5a4fbb-315d-4fbc-a7f9-fe3915c73ed1">

오브젝트가 갱신될 때마다, 서버는 오브젝트를 감시하고 있는 연결된 모든 클라이언트에게 오브젝트의 새로운 버전을 보낸다.  

``k get pods --watch``로 생성되고 삭제되는 파드를 감시할 수 있다.  

<Br/>

### 스케줄러 이해

파드가 어떤 노드에 올라갈 것인지는 스케줄러가 담당핳ㄴ다. API 서버의 감시 메커니즘을 통해 새로운 파드를 기다리고 있다가 할당된 노드가 없는 새로운 파드를 노드에 할당하기만 한다.  

선택된 노드에 파드를 실행하도록 지시하지 않고 API 서버로 파드 정의를 갱신한다. API 서버는 Kubelet에 파드가 스케줄링된 것을 통보한다(앞에서 설명한 감시 메커니즘을 통해). 

<img width="766" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/ad5b8882-1ace-43c2-b565-42ffff2d89a9">

수용 가능한 노드를 찾을 때에 하드웨어 리소스에 대한 파드 요청을 충족시킬 수 있는지, 노드가 파드 정의 안에 있는 노드 셀렉터와 일치하는 레이블을 가지고 있는지 등등의 모든 검사를 통과해야만 한다. 이에 부합하는 노드가 여러대라면 이중 적합한 노드를 선택한다.  

파드의 레플리카가 여러개라면 가능한 많은 노드에 분산시키는 것이 이상적이다. 동일한 서비스또는 레플리카셋에 속한 파드는 기본적으로 여러 노드에 분산된다. 항상 그런 것은 아니지만, 어피니티와 안티 어피니티 규칙을 정의해 클러스터 전체에 퍼지거나 가깝게 유지되도록 강제할 수 있다.  

클러스터에서 여러 개의 스케줄러를 실행할 수도 있다. 파드 정의에 schedulerName을 두어서 스케줄러를 선택할 수도 있다. 이 속성을 설정하지 않은 파드는 기본적으로 기본 스케줄러를 사용한다. 스케줄러명을 default-scheduler로 설정한 파드 또한 마찬가지다.  

<Br/>

### 컨트롤러 매니저에서 실행되는 컨트롤러 소개

API 서버에 배포된 리소스에 지정된 대로 시스템을 원하는 상태로 수렴되도록 하는 활성 구성 요소로 컨트롤러 매니저 안에서 실행되는 컨트롤러에 의해 수행된다. 

* 레플리케이션 매니저(레플리케이션컨트롤러 리소스의 컨트롤러)
* 레플리카셋, 데몬셋, 잡 컨트롤러
* 디플로이먼트 컨트롤러
* 서비스 컨트롤러

등등 각 컨트롤러들은 API 서버에서 리소스가 변경되는 것을 감지하고 각 변경작업을 수행한다. 대부분 이러한 작업은 다른 리소스 생성, 감시 중인 리로스 자체를 갱신하는 것이 포함된다. 컨트롤러는 서로 대화하지 않고 다른 컨트롤러가 존재하는지도 모른다. 각 컨트롤러는 API 서버에 연결하고 감시 메커니즘을 통해 컨트롤러가 담당하는 리소스 유형에서 변경이 발생하면 통보해줄 것을 요청한다.  

<br/>

### Kubelet이 하는 일

Kubelet이 실행 중인 노드를 노드 리소스로 만들어서 API 서버에 등록하는 것이 첫 번째 일이다. 그런 다음 API 서버를 지속적으로 모니터링해 해당 노드에 파드가 스케줄링되면, 파드의 컨테이너를 시작한다. 설정된 컨테이너 런타임에 지정된 컨테이너 이미지로 컨테이너를 실행하도록 지시함으로써 이 작업을 수행한다. 그런 다음 Kubelet은 실행 중인 컨테이너를 계속 모니터링하면서 상태, 이벤트, 리소스 사용량을 API 서버에 보고한다.  

Kubelet은 컨테이너 라이브니스 프로브를 실행하는 구성 요소이기도 하며, 프로브가 실패할 경우 컨테이너를 다시 시작한다. 마지막으로 API 서버에서 파드가 삭제되면 컨테이너를 정지하고 파드가 종료된 것을 서버에 통보한다.  

<img width="720" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/abf8628e-8adc-4f6c-a463-f174d6d47301">

API 서버를 이용하여 파드 매니페스트를 가져오는 방법 외에 그림과 같이 특정 로컬 디렉터리 안에 있는 파일을 기반으로 파드를 실행할 수도 있다.  

쿠버네티스 시스템 구성 요소가 기본적으로 실행되도록 하는 대신, 시스템 구성 요소 파드 매니페스트를 Kubelet의 매니페스트 디렉터리 안에 넣어서 Kubelet이 실행하고 관리하도록 할 수 있다.  

<br/>

### 쿠버네티스 서비스 프록시의 역할

kube-proxy의 초기 구현은 사용자 공간에서 동작하는 프록시였다. 실제 서버가 프로세스가 연결을 수락하고 이를 파드로 전달했다. 서비스 IP로 향하는 연결을 가로채기 위해 프록시는 iptables 규칙을 설정해 이를 프록시 서버로 전송했다. 

<img width="746" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/f266670d-ba60-4d4a-8133-f40eeb6b9196">

현재는 iptables 규칙만 사용해 프록시 서버를 거치지 않고 패킷을 무작위로 선택한 백엔드 파드로 전달한다.  

<img width="743" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/c99e2f8b-77da-4ac9-9104-230135d45a50">

<br/>

## 컨트롤러가 협업하는 방법

<img width="744" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/ea2c1c14-8a96-40ca-9e1b-0cd29e1666cd">

컨트롤러와 스케줄러 그리고 Kubelet은 API 서버에서 각 리소스 유형이 변경되는 것을 감시한다.  

<img width="849" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/e221dded-6d37-4c8c-a81f-d519fa371714">

kubectl 명령으로 디플로이먼트 yaml을 쿠버네티스에 게시하게 되면 위와 같이 이벤트들이 연계된다.  

<br/>

### 클러스터 이벤트 관찰

구성요소와 Kubelet은 위와 같은 작업을 수행할 때 API 서버로 이벤트를 발송하는데 다른 리소스들과 마찬가지로 이벤트 리소스를 만들어 이를 수행한다.  

``kubectl get events --watch``를 통해 컨트롤러가 생성한 이벤트를 관찰할 수 있다.  

<br/>

## 실행 중인 파드에 관한 이해

<img width="709" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/89c8bdc5-cbae-4bc2-9f71-268e54d21791">

퍼즈 컨테이너는 파드의 모든 컨테이너를 함께 담고 있는 컨테이너다. 퍼즈 컨테이너는 동일한 네트워크와 리눅스 네임스페이스를 모두 보유하는게 유일한 목적인 인프라스트럭처 컨테이너다.  

<Br/>

## 파드 간 네트워킹

<img width="680" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/0a0d6e88-f087-4d57-89e5-c5e1690490e2">

파드A가 바라보는 파드B의 IP주소는 파드B의 IP 주소와 일치해야 한다. 파드 사이에 NAT가 없으면 내부에서 실행 중인 애플리케이션이 다른 파드에 자동으로 등록되도록 할 수 있다. 파드 간에 NAT 없이 통신해야 한다는 요구 사항은 파드와 노드 그리고 노드와 파드 간에 통신할 때 동일하게 요구된다.  그러나 파드가 인터넷에 있는 서버스와 통신할 때는 패킷의 출발지 IP를 변경하는 것이 필요하다. 파드의 IP는 사설이기 때문이다. 외부로 나가는 패킷의 출발지 IP는 호스트 워커 노드의 IP로 변경된다.  

<Br/>

## 서비스 구현 방식

### kube-proxy 소개

서비스와 관련된 모든 것은 각 노드에서 동작하는 kube-proxy 프로세스에 의해 처리된다. 초기에는 kube-proxy가 실제 프록시로서 연결을 기다리다가, 들어온 연결을 위해 해당 파드로 가는 새로운 연결을 생성했다. 이것을 userspace 프록시 모드라고 한다. 나중에는 성능이 더 우수한 iptables 프록시 모드가 이를 대체했다.  

각 서비스는 안정적인 IP 주소와 포트를 얻는다. 클라이언트는 이를 이용해 서비스에 접속해 사용한다. 이 IP 주소는 가상이다. 어떠한 네트워크 인터페이스에도 할당되지 않고 패킷이 노드를 떠날 때 네트워크 패킷안에 출발지 혹은 도착지 IP 주소로 표시되지 않는다. 서비스의 주요 핵심 사항은 서비스가 IP와 포트의 쌍으로 구성된다는 것으로, 서비스 IP만으로는 아무것도 나타내지 않는다. 이게 서비스에 핑을 보낼 수 없는 이유다.  

<Br/>

### kube-proxy가 iptables를 사용하는 방법

API 서버에서 서비스를 생성하면, 가상 IP 주소가 바로 할당된다. 곧이어 API 서버는 워커 노드에서 실행 중인 모든 kube-proxy 에이전트엥 새로운 서비스가 생성됐음을 통보한다. 각 kube-proxy는 실행 중인 노드에 해당 서비스 주소로 접근할 수 있도록 만든다. 이것은 서비스의 IP/포트 쌍으로 향하는 패킷을 가로채서, 목적지 주소를 변경해 패킷이 서비스를 지원하는 여러 파드 중 하나로 리디렉션되도록 하는 몇 개의 iptables 규칙을 설정함으로써 이뤄진다.  

kube-proxy는 API 서버에서 서비스가  변경되는 것을 감지하는 것 외에도, 엔드포인트 오브젝트가 변경되는 것을 같이 감시한다. 

<img width="640" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/ddc85535-4a8f-4b72-8f02-6cd9fbb80491">

처음에 패킷의 목적지는 서비스의 IP와 포트로 지정된다. 패킷이 네트워크로 전송되기 전에 노드 A의 커널이 노드에 설정된 iptables 규칙에 따라 먼저 처리한다. 커널은 패킷이 iptables 규칙 중에 일치하는게 있는지 검사한다. 그 규칙 중 하나에서 패킷 중에 목적지 IP가 172.30.0.1이고 목적지 포트가 80인 포트가 있다면, 임의로 선택된 파드의 IP와 포트로 교체돼야 한다고 알려준다.  

<br/>

## 고가용성 클러스터 실행

### 애플리케이션 가용성 높이기

디플로이먼트 리소스로 애플리케이션을 실행하고 적절한 수의 레플리카를 설정하면 애플리케이션의 가용성이 높아진다.  

가동 중단 시간을 줄이기 위해 다중 인스턴스로 실행한다. 수평 스케일링이 불가능한 애플리케이션을 위해 리더 선출 매커니즘을 사용한다.  

<br/>

### 쿠버네티스 컨트롤 플레인 구성 요소의 가용성 향상

<img width="815" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/c89448a1-08c4-4688-ab4f-0cf4249c088b">

* etcd 클러스터 실행
  * etcd는 분산 시스템으로 설계됐으므로 그 주요 기능은 여러 etcd 인스턴스를 실행하는 기능이라, 가용성을 높이는 것은 큰 문제가 되지 않는다.
* 여러 API 서버 인스턴스 실행
  * API 서버는 상태를 저장하지 않기 때문에, 필요한 만큼 많은 API 서버를 실행할 수 있고 서로 인지할 필요도 없다.
  * 일반적으로 모든 etcd 인스턴스에 API 서버를 함께 띄운다.
* 컨트롤러와 스케줄러 고가용성 확보
  * 컨트롤러와 스케줄러는 클러스터 상태를 감시하고 상태가 변경될 때 반응해야 하는데 이런 구성 요소의 여러 인스턴스가 동시에 실행돼 같은 동작을 수행하면, 클러스터 상태가 예상보다 더 많이 변경될 가능성이 있다. 따라서 하나의 인스턴스만 활성화하고 나머지는 현재 활성화된 리더가 실패할 경우 새로운 리더 선출을 통해 동작하게 된다. 

<img width="773" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/61927b31-ff3c-4b12-bbb8-942a273bfb94">

리더 선출 메커니즘은 API 서버에 오브젝트를 생성하는 것만으로 완전히 동작한다. 

---

