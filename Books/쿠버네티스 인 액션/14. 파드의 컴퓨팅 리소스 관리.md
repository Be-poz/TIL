# 파드의 컴퓨팅 리소스 관리

## 파드 컨테이너의 리소스 요청

파드를 생성할 때 컨테이너가 필요로 하는 CPU와 메모리 양과 사용할 수 있는 엄격한 제한을 지정할 수 있다.  

<br/>

### 리소스 요청을 갖는 파드 생성하기

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: requests-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main									# 주 컨테이너에 리소스 요청을 지정한다.
    resources:
      requests:
        cpu: 200m								# 컨테이너는 200밀리코어를 요청한다. (하나의 CPU 코어 시간의 1/5)
        memory: 10Mi						# 컨테이너는 10Mi(Mebibyte)의 메모리를 요청한다.
```

1/5 CPU 코어를 필요로 한다고 적어놨는데 이런 파드/컨테이너 5개를 CPU 코어 하나에서 충분히 빠르게 실행할 수 있다.  

CPU 요청을 지정하지 않으면 컨테이너에서 실행 중인 프로세스에 할당되는 CPU 시간에 신경 쓰지 않는다는 것과 같다. 최악의 경우 CPU 시간을 전혀 할당받지 못할 수 있다. 시간이 중요하지 않은 우선순위가 낮은 배치 작업은 괜찮지만 사용자 요청을 처리하는 컨테이너에는 분명 적합하지 않다.  

10Mi의 메모리를 요청함으로써 컨테이너 내부에 실행 중인 프로세스가 최대 10Mi의 메모리를 사용할 것을 예상할 수 있다.  

파드 생성 후 ``k exec -it requests-pod top``을 실행하여 프로세스의 CPU 소비량을 확인할 수 있다.  

<br/>

### 리소스 요청이 스케줄링에 미치는 영향

리소스 요청을 하면 스케줄러는 충분한 리소스를 가진 노드만을 고려한다. 부합하지 않은 노드에는 스케줄링 하지 않는다.  

#### 파드가 특정 노드에 실행할 수 있는지 스케줄러가 결정하는 방법

스케줄러는 스케줄링하는 시점에 각 개별 리소스가 얼마나 사용되는지 보지 않고, 노드에 배포된 파드들의 리소스 요청량의 전체 합만을 본다는 것이다.  
파드가 요청한 것보다 적게 사용할지라도 실제 리소스 사용량에 기반해 다른 파드를 스케줄링한다는 것은 이미 배포된 파드에 대한 보장을 깨뜨릴 수 있다.  

<img width="812" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/9bf359be-404a-4527-8306-b9bda2593a40">

#### 스케줄러가 파드를 위해 최적의 노드를 선택할 때 파드의 요청을 사용하는 방법

11장에서 스케줄러는 파드에 맞지 않은 노드를 제거하기 위해 노드의 목록을 필터링한 다음 설정된 우선순위 함수에 따라 남은 노드의 우선순위를 지정한다고 했다. 다른 여러 우선순위 함수 중에서, 두 개의 우선순위 함수가 요청된 리소스 양에 기반해 노드의 순위를 정한다.  

LeastRequestedPriority와 MostRequestPriority이다. 첫 번째 함수는 요청된 리소스가 낮은 노드를 선호하는 반면, 두 번째 함수는 그와 정반대로 요청된 리소스가 가장 많은 노드를 선호한다.  

스케줄러는 이들 함수 중 하나만을 이용하도록 설정된다. 일반적으로 노드들의 세트가 있는 경우 CPU 부하를 전체 노드에 고르게 분산하기를 원할 것이다.  
하지만 필요하면 언제든 노드를 추가하거나 제거할 수 있는 클라우드 인프라에서 실행하는 경우는 다르다. 스케줄러가 MostRequestedPriority 함수를 사용하도록 설정하면 쿠버네티스는 파드가 요청한 CPU와 메모리 양을 제공하면서도 가장 적은 수의 노드를 사용하도록 보장한다. 파드를 일부 노드에 많이 스케줄링해 특정 노드를 비울 수 있고 제거할 수 있다. 각 노드별로 비용을 지불하므로 이렇게 비용을 절감할 수 있다.  

#### 노드의 용량 검사

``k describe nodes``로 노드 리소스를 확인한다. ``Capacity.memory``에서 노드의 전체 용량을 확인할 수 있고, ``Allocatable.memory``를 통해 파드에 할당 가능한 리소스를 확인할 수 있다. 이제 cpu 800m, memory 20Mi의 파드를 하나 더 생성해보자.  

#### 어느 노드에도 실행할 수 없는 파드 생성

현재 2개의 파드가 배포됐고 총 1000 밀리코어, 정확히 1코어를 요청했다. 따라서 추가적인 파드에 1,000 밀리코어가 사용 가능해야 한다.  

이제 cpu 1, memory 20Mi인 3번째 파드를 배포해보자. k get po를 해보면 Pending 상태인 것을 확인할 수 있을 것이고 describe 해보면 cpu가 부족해 스케줄링이 실패했다는 문구를 확인할 수가 있다.  

#### 파드가 스케줄링되지 않은 이유 확인

<img width="696" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/ff733d25-c978-4e27-8dfc-3e2780b81790">

요청한 CPU는 1000m인데 1275m으로 275밀리코어가 더 많다. 확인해보면 kube-system 네임스페이스의 3개의 파드 때문인 것을 알 수가 있다.  

#### 파드가 스케줄링될 수 있도록 리소스 해제

파드는 적절한 양의 CPU가 남아 있는 경우에만 스케줄링이 된다. 2번째 파드를 삭제하면 스케줄러는 삭제를 통지받고 두 번째 파드가 종료되자마자 세 번째 파드를 스케줄링할 것이다.  

<br/>

### CPU 요청이 CPU 시간 공유에 미치는 영향

<img width="723" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/08ea3750-8863-4ccd-8536-090b2f39f1b9">

위와 같이 동작한다. 그리고 한 컨테이너가 CPU를 최대로 사용하려는 순간 나머지 파드가 유휴 상태에 있다면 첫 번째 컨테이너가 전체 CPU 시간을 사용할 수 있다. 결국 아무도 사용하지 않는다면 사용 가능한 모든 CPU를 사용하는 것이 상식이다.  

<br/>

### 사용자 정의 리소스의 정의와 요청

쿠버네티스를 사용하면 사용자 정의 리소스를 노드에 추가하고 파드의 리소스 요청으로 사용자 정의 리소스를 요청할 수 있다.  

먼저 노드 오브젝트의 capacitty 필드에 값을 추가해 쿠버네티스가 사용자 정의 리소스를 인식하도록 해야 한다. 이는 PATCH HTTP 요청을 수행해 이뤄진다. 리소스 이름은 kubernetes.io 도메인으로 시작하지 않는 이상 example.org/my-resource와 같이 무엇이든 될 수 있다. 수량은 반드시 정수여야 한다. 이 값은 capacity 필드에서 allocatable 필드로 자동으로 복사된다.  

그런 다음 파드를 생성할 때 동일한 리소스 이름과 수량을 컨테이너 스펙의 resources.requests 필드로 지정하거나 이전 예제와 같이 kubectl run에 --requests를 사용해 지정한다.  

사용자 정의 리소스의 예로는 노드에 사용 가능한 GPU 단위 수가 있다. GPU 사용을 요구하는 파드는 요청에 이를 지정한다. 스케줄러는 할당되지 않은 GPU가 적어도 하나가 있는 노드에만 파드가 스케줄링되도록 보장한다.  

<br/>

## 컨테이너에 사용 가능한 리소스 제한

### 컨테이너가 사용 가능한 리소스 양을 엄격한 제한으로 설정

다른 모든 프로세스가 유휴 상태일 때 컨테이너가 남은 모든 CPU를 사용하는 방법을 살펴봤다. 그러나 특정 컨테이너가 지정한 CPU 양보다 많은 CPU를 사용하는 것을 막고 싶을 수 있다. 그리고 컨테이너가 사용하는 메모리 양을 제한하고 싶을 수도 있다.  

CPU는 압축 가능한 리소스다. 즉, 컨테이너에서 실행 중인 프로세스에 부정적인 영향을 주지 않고 컨테이너가 사용하는 CPU 양을 조절할 수 있따. 메모리는 압축이 불가능하다. 프로세스에 메모리가 주어지면 프로세스가 메모리를 해제하지 않는 한 가져갈 수 없다. 그것이 컨테이너에 할당되는 메모리의 최대량을 제한해야 하는 이유다. 메모리를 제한하지 않으면 워커 노드에 실행중인 컨테이너는 사용 가능한 모든 메모리를 사용해서 노드에 있는 다른 모든 파드와 노드에 스케줄링되는 새 파드에 영향을 미칠 수 있다. 오작동하거나 악의적인 파드 하나가 실제 전체 노드를 사용할 수 없게 만들 수 있다.  

#### 리소스 제한을 갖는 파드 생성

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: limited-pod
spec:
  containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:				# 컨테이너의 리소스 제한을 지정한다.
        cpu: 1			# 이 컨테이너는 최대 CPU 1코어를 사용할 수 있다.
        memory: 20Mi # 컨테이너는 최대 메모리 20Mi를 사용할 수 있다.
```

#### 리소스 제한 오버커밋

리소스 요청과는 달리 리소스 제한은 노드의 할당 가능한 리소스 양으로 제한되지 않는다.  
노드에 있는 모든 파드의 리소스 제한 합계는 노드 용량의 100%를 초과할 수 있다. 다시 말하면 리소스 제한은 오버커밋 될 수 있다.  

<img width="773" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/3b04f82e-c6ba-429f-b6fc-6527dc61a614">

<br/>

### 리소스 제한 초과

컨테이너에서 실행 중인 프로세스가 허용된 양보다 많은 리소스를 사용하려고 하면 어떤일이 발생할까?  

프로세스의 CPU 사용률은 조절되므로 컨테이너에 CPU 제한이 설정돼 있으면 프로세스는 설정된 제한보다 많은 CPU 시간을 할당받을 수 없다.  

메모리는 CPU와는 다르다. 프로세스가 제한보다 많은 메모리를 할당받으려 시도하면 프로세스는 종료된다. 파드의 재시작 정책이 Always 또는 OnFailure로 설정된 경우 프로세스는 즉시 다시 시작하므로 종료됐음을 알아차리지 못할 수 있다, 하지만 메모리 제한 초과와 종료가 지속되면 쿠버네티스는 재시작 사이의 지연 시간을 증가시키면서 재시작시킨다. 이런 경우 ``CrashLoopBackOff`` 상태가 표시된다.  

이 상태는 kubelet이 포기한 것이 아니라 각 크래시 후 kubelet이 컨테이너를 다시 시작하기 전에 간격을 늘리는 것을 의미한다.  

<br/>

### 컨테이너의 애플리케이션이 제한을 바라보는 방법

위의 limited-pod를 실행하고 ``k exec -it limited-pod top`` 으로 내부 지표를 살펴보면 used 메모리와 free 메모리 양이 20Mi와는 거리가 멀다는 것을 확인할 수가 있다. 비슷하게 CPU 또한 다르다.  왜 이런 것일까?

#### 컨테이너는 항상 컨테이너 메모리가 아닌 노드 메모리를 본다

top 명령은 컨테이너가 실행 중인 전체 노드의 메모리 양을 표시한다. 컨테이너에 사용 가능한 메모리의 제한을 설정하더라도 컨테이너는 이 제한을 인식하지 못한다. 이는 시스템에서 사용 가능한 메모리 양을 조회하고 해당 정보를 사용해 예약하려는 메모리 양을 결정하는 모든 애플리케이션에 좋지 않은 영향을 미친다. 

자바 애플리케이션을 실행할 때 특히 -Xms 옵션으로 JVM의 최대 힙 크기를 지정하지 않은 경우 문제가 발생한다. 이런 경우 JVM은 컨테이너에 사용 가능한 메모리 대신 호스트의 총 메모리를 기준으로 최대 힙 크기를 설정할 것이다. 많은 물리 메모리를 갖는 프로덕션 환경에 파드를 배포하면 JVM이 컨테이너에 설정된 메모리 제한을 초과해 OOMKilled가 될 수 있다.  

적절한 -Xms 옵션을 설정해도 해결되지 않는다. -Xms 옵션은 힙 크기를 제한하지만 JVM의 오프 힙 메모리에는 영향을 미치지 않는다. 다행히도 새 버전의 자바는 컨테이너 제한을 설정한 것을 고려해 문제를 완화한다.  

#### 컨테이너는 또한 노드의 모든 CPU 코어를 본다

메모리와 마찬가지로 컨테이너는 컨테이너에 결정된 CPU 제한과 상관없이 노드의 모든 CPU를 본다. CPU 제한을 1코어로 설정하는 것은 마법과 같이 컨테이너에 CPU 1코어만을 노출하지 않는다. CPU 제한이 하는 일은 컨테이너가 사용할 수 있는 CPU 시간의 양을 제한하는 것이다.  

어떤 애플리케이션은 시스템의 CPU 수를 검색해 실행해야 할 작업 스레드 수를 결정한다. 이런 애플리케이션은 개발 노트북에서는 정상적으로 돌아가지만 많은 수의 코어를 갖는 노드에 배포하면 너무 많은 스레드가 기동돼 제한된 CPU 시간을 두고 모두 경합하게 된다. 또한 각 스레드는 추가적인 메모리를 요구하게 돼 애플리케이션의 메모리 사용량이 급증한다.  

<Br/>

## 파드 QoS 클래스 이해

리소스 제한은 오버커밋될 수 있으므로 노드가 모든 파드의 리소스 제한에 지정된 양의 리소스를 반드시 제공할 수는 없다고 언급했다.  

2개의 파드가 있다고 가정할 때, 파드 A는 노드 메모리의 90%를 사용하고 있는 상황에서 파드 B가 갑자기 그 시점까지 사용하던 메모리보다 많은 메모리를 요구해 노드가 필요한 양의 메모리를 제공할 수 없다면 어떤 컨테이너를 종료해야 할까? 상황에 따라 다르다. 쿠버네티스는 파드를 세 가지 서비스 품질클래스로 분류한다.  

* BestEffort (최하위 우선순위)
* Burstable
* Guaranteed (최상위 우선순위)

<br/>

### 파드의 QoS 클래스 정의

#### BestEffort 클래스에 파드를 할당하기

우선순위가 가장 낮은 클래스다. 아무런 리소스 요청과 제한이 없는 파드에 할당된다. 이런 파드에 실행 중인 컨테이너는 리소스 보장을 받지 못한다.  
최악의 경우 CPU 시간을 전혀 받지 못할 수 있고 다른 파드를 위해 메모리가 해제돼야 할 때 가장 먼저 종료된다. 그러나 BestEffort 파드는 설정된 메모리 제한이 없으므로 메모리가 충분한다면 컨테이너는 원하는 만큼 메모리를 사용할 수 있다.  

#### Guaranteed 클래스에 파드를 할당하기

이 클래스는 모든 리소스를 컨테이너의 리소스 요청이 리소스 제한과 동일한 파드에게 주어진다. 해당 클래스이기 위해서는 3가지를 충족해야 한다.

* CPU와 메모리에 리소스 요청과 제한이 모두 설정돼야 한다.
* 각 컨테이너에 설정돼야 한다.
* 리소스 요청과 제한이 동일해야 한다.

컨테이너의 리소스 요청이 명시적으로 설정되지 않은 경우 기본적으로 리소스 제한과 동일하게 설정되므로 모든 리소스에 대한 제한을 지정하는 것으로 파드가 Guaranteed가 되기 충분하다. 이런 파드의 컨테이너는 요청된 리소스의 양을 얻지만 추가 리소스를 사용할 수 없다.

#### Burstable QoS 클래스에 파드를 할당하기

BestEffort와 Guaranteed 사이가 Burstable QoS 클래스다. 그 밖의 다른 파드는 이 클래스에 해당한다. 컨테이너의 리소스 제한이 리소스 요청과 일치하지 않은 단일 컨테이너 파드와 적어도 한 개의 컨테이너가 리소스 요청을 지정했지만 리소스 제한을 설정하지 않은 모든 파드가 여기에 속한다. 또한 컨테이너 하나의 리소스 요청과 제한은 일치하지만 다른 컨테이너의 리소스 요청과 제한을 지정하지 않는 파드도 포함된다. Burstable 파드는 요청한 양 만큼의 리소스를 얻지만 필요하면 추가 리소스를 사용할 수 있다.

<img width="629" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/b4410c38-36b6-4c0a-b185-d93988168607">

<img width="608" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/e6dd2114-7909-4202-968c-a6f3b4a20b59">
<img width="639" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/d0a7448a-1660-4cb4-8709-61bb0211120f">

<Br/>

### 메모리가 부족할 때 어떤 프로세스가 종료되는지 이해

QoS 클래스는 어떤 컨테이너를 먼저 종료할지 결정하고 해제된 리소스를 높은 우선순위의 파드에 줄 수 있다. BestEffort 클래스가 가장 먼저 종료되고 다음은 Burstable 파드가 종료되며, 마지막으로 Guaranteed 파드는 시스템 프로세스가 메모리를 필요로 하는 경우에만 종료된다.  

<img width="870" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/bc3f901a-78a2-4375-ac5f-260069ac119d">

동일한 QoS 클래스인 경우에는 시스템은 요청된 메모리의 비율이 달느 것보다 높은 컨테이너를 종료한다. 위 그림에서 요청된 메모리의 90%를 사용하는 파드 B가 70ㅖ%를 사용하는 파드 C 보다 먼저 종료된 이유다.  

<Br/>

## 네임스페이스별 파드에 대한 기본 요청과 제한 설정

### LimitRange 리소스 소개

모든 컨테이너에 리소스 요청과 제한을 설정하는 대신 LimitRange 리소스를 생성해 이를 수행할 수 있다. LimitRange 리소스는 컨테이너의 각 리소스에 최소/최대 제한을 지정할 뿐만 아니라 리소스 요청을 명시적으로 지정하지 않은 컨테이너의 기본 리소스 요청을 지정한다.  

<img width="786" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/ff5d8b69-281a-40cb-9ac8-fc72e670272d">

LimitRange 리소스는 LimitRanger 어드미션 컨트롤 플러그인에서 사용된다. 파드 매니페스트가 API 서버에 게시되면 LimitRanger 플러그인이 파드 스펙을 검증한다. 검증이 실패하면 매니페스트는 즉시 거부된다. 이 때문에 LimitRanger 오브젝트의 좋은 사용 사례는 클러스터의 어느 노드보다 큰 파드를 생성하려는 사용자를 막는 것이다. 이런 LimitRange가 없으면 API 서버는 절대 스케줄링되지 않는 파드라도 기꺼이 받아들인다.  

<br/>

### LimitRange 오브젝트 생성하기

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: example
spec:
  limits:
  - type: Pod			# 파드 전체에 리소스 제한을 지정
    min:
      cpu: 50m		# 모든 파드의 컨테이너가 전체적으로 요청할 수 있는 최소 CPU 및 메모리
      memory: 5Mi
    max:
      cpu: 1			# 모든 파드의 컨테이너가 요청하는 최대 CPU 및 메모리
      memory: 1Gi
  - type: Container	# 컨테이너 제한은 이 줄의 아래에 지정된다
    defaultRequest:	# 명시적으로 요청을 지정하지 않은 컨테이너에 적용되는 CPU 및 메모리의 요청
      cpu: 100m
      memory: 10Mi
    default:				# 리소스 제한을 지정하지 않은 컨테이너의 기본 제한
      cpu: 200m
      memory: 100Mi
    min:						# 컨테이너가 가질 수 있는 최소 및 최대 요청/제한
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
    maxLimitRequestRatio:	# 각 리소스의 제한과 요청 간의 최대 비율
      cpu: 4
      memory: 10
  - type: PersistentVolumeClaim	# LimitRange는 PVC가 요청할 수 있는 스토리지의 최소 및 최대량을 설정할 수 있다.
    min:
      storage: 1Gi
    max:
      storage: 10Gi
```

<br/>

### 강제 리소스 제한

제한이 설정되면 이제 LimitRange에서 허용하는 것보다 더 많은 CPU를 요청하는 파드를 만들 수 있따.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: too-big
spec:
  containers:
  - image: busybox
    args: ["sleep", "9999999"]
    name: main
    resources:
      requests:
        cpu: 2
```

LimitRange의 최대보다 큰 2개의 CPU를 요청하는 파드를 생성하려고 하면  

```shell
The Pod "too-big" is invalid: spec.containers[0].resources.requests: Invalid value: "2": must be less than or equal to cpu limit
```

이러한 오류가 발생한다. 파드가 거부된 것이다. 

<br/>

### 기본 리소스 요청과 제한 적용

리소스 요청과 제한이 지정되지 않은 파드를 생성 후 describe 해보면 LimitRange에 설정한 값이 들어간 것을 확인할 수가 있다.  

![image](https://github.com/Be-poz/TIL/assets/45073750/38597207-63c0-4da7-891e-666937977bd0)

<br/>

## 네임스페이스의 사용 가능한 총 리소스 제한하기

LimitRange를 이용해서 개별 파드에 대한 적용을 했지만 네임스페이스에서 사용 가능한 총 리소스 양을 제한할 수 있는 방법이 필요하다.  
이것을 ResourceQuota, 리소스쿼터 오브젝트를 생성해 달성할 수 있다.  

<Br/>

### 리소스쿼터 오브젝트 소개

리소스쿼터는 네임스페이스에서 파드가 사용할 수 있는 컴퓨팅 리소스 양과 퍼시스턴트볼륨클레임이 사용할 수 있는 스토리지 양을 제한한다. 또한 네임스페이스 안에서 사용자가 만들 수 있는 파드, 클레임, 기타 API 오브젝트의 수를 제한할 수 있다.  

#### CPU 및 메모리에 관한 리소스쿼터 생성

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: cpu-and-mem
spec:
  hard:
    requests.cpu: 400m
    requests.memory: 200Mi
    limits.cpu: 600m
    limits.memory: 500Mi
```

각 리소스의 합계를 정의하는 대신 CPU 및 메모리에 대한 요청과 제한에 대한 별도의 합계를 정의한다. LimitRange의 구조와 비교해 구조가 약간 다르다. 여기에서 모든 리소스에 관한 요청과 제한을 한 곳에서 정의한다.  

<img width="776" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/61c3649a-ded2-4dcf-877f-e9180f14c9ac">

#### 쿼터와 쿼터 사용량 검사

``k describe quota``로 리소스쿼터를 검사해보면 상태가 나온다. Used는 현재 사용된 리소스 요청과 제한이다.  

#### 리소스쿼터와 함께 LimitRange 생성

리소스쿼터를 생성할 때 주의할 점은 LimitRange 오브젝트도 함께 생성해야 한다는 것이다. 특히 리소스에 대한 쿼터가 설정된 경우, 파드에는 동일한 리소스에 대한 요청 또는 제한이 설정돼야 한다. 그렇지 않으면 API 서버가 파드를 허용하지 않는다. 그렇기 때문에 이러한 리소스에 대한 기본값이 있는 LimitRange를 갖는 것이 파드를 만드는 것을 조금 더 쉽게 만든다.  

<br/>

### 퍼시스턴트 스토리지에 관한 쿼터 지정하기

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage
spec:
  hard:
    requests.storage: 500Gi
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi
    standard.storageclass.storage.k8s.io/requests.storage: 1Ti
```

<Br/>

### 생성 가능한 오브젝트 수 제한

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: objects
spec:
  hard:
    pods: 10
    replicationcontrollers: 5
    secrets: 10
    configmaps: 10
    persistentvolumeclaims: 5
    services: 5
    services.loadbalancers: 1
    services.nodeports: 2
    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2
```

리소스쿼터는 네임스페이스 내의 파드, 레플리케이션컨트롤러, 서비스 및 그 외의 오브젝트 수를 제한하도록 구성할 수 있다.  

<br/>

### 특정 파드 상태나 QoS 클래스에 대한 쿼터 지정

쿼터는 쿼터 범위를 BestEffort, NotBestEffort, Terminating, NotTerminating의 네 가지 범위를 사용할 수 있다.  

Terminating, NotTerminating은 종료 과정의(또는 종료되지 않은)파드에는 적용되지 않는다. 아직 다루지는 않았지만, 각 파드가 종료되고 실패로 표시되기 전에 얼마나 오래 실행할 수 있는지 지정할 수 있다. 파드 스펙의 activeDeadlineSeconds 필드는 파드가 실패로 표시된 후 종료되기 전 시작 시간을 기준으로 노드에서 활성화되도록 허용하는 시간을 정의한다. Terminating 쿼터 범위는 activeDeadlineSeconds가 설정된 파드에 적용되는 반면 NotTerminating은 그렇지 않은 파드에 적용된다.  

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  scopes:					# 이 쿼터는 BestEffort QoS를 갖고 유효 데드라인이 없는 파드에만 적용된다.
  - BestEffort
  - NotTerminating
  hard:
    pods: 4				# 그러한 파드는 네 개만 존재할 수 있다.
```

이 쿼터는 유효 데드라인(activeDeadline)이 없는 BestEffort QoS 클래스를 갖는 파드가 최대 4개 존재하도록 보장한다. 그 대신 쿼터가 NotBestEffort 파드를 대상으로 하는 경우  request.cpu, request.memory, limit.cpu, limit.memory도 지정할 수 있다.  

<br/>

## 파드 리소스 사용량 모니터링

### 실제 리소스 사용량 수집과 검색

쿠버네티스에서 실행 중인 애플리케이션 모니터링은 Kubelet 자체에 cAdvisor 라는 에이전트가 포함돼 있고 이 에이전트가 노드에서 실행되는 개별 컨테이너와 노드 전체의 리소스 사용 데이터를 수집한다. 전체 클러스터를 이러한 통계를 중앙에서 수집하려면 힙스터라는 추가 구성 요소를 실행해야 한다(최근 쿠버네티스에서는 힙스터 대신 메트릭 서버를 사용한다).  

<img width="742" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/07713c97-9184-405d-880c-0a7b0ed19a95">

노드 중 하나에서 파드로 실행되는 힙스터는 일반적인 쿠버네티스 서비스를 통해 노출돼 안정된 IP 주소로 접속할 수 있다.  

파드는 cAdvisor를 전혀 모르고 cAdvisor는 힙스터를 전혀 모른다. 힙스터가 모든 cAdvisor에 연결하며, cAdvisor는 파드의 컨테이너 내부에서 실행 중인 프로세스와 통신하지 않고 컨테이너와 노드의 사용량 데이터를 수집한다.  

힙스터가 실행되면 ``k top node``와 같은 명령어로 노드에서 사용 중인  CPU 및 메모리 양을 확인 가능하다.  

---



