# 카프카의 내부 동작 원리와 구현

토픽 생성 시 레플리케이션을 사용하기 위해서는 ``replication factor`` 옵션을 설정해야 한다.  

```sh
sh kafka-topics.sh --bootstrap-server {server-name} --create --topic {topic-name} --partitions 1 --replication-factor 3
```

server를 작성할 때에는 카프카 클러스터에 연결하기 위한 초기 연결점이기 때문에 모든 브로커들을 작성할 필요는 없다.  

```sh
sh kafka-topics.sh --bootstrap-server {server-name} --topic {topic-name} --describe

Topic: peter-test01	PartitionCount: 1	ReplicationFactor: 3	Configs: segment.bytes=1073741824
	Topic: peter-test01	Partition: 0	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2	
```

생성 후 describe 옵션을 이용하여 토픽을 살펴보면 위와 같이 결과가 나온다.  
파티션 개수와 레플리케이션 개수가 나오고, 어떤 브로커가 리더인지 나온다.  

```sh
sh kafka-topics.sh --bootstrap-server {server-name} --topic {topic-name} --describe

Topic: poz-test02	PartitionCount: 2	ReplicationFactor: 3	Configs: segment.bytes=1073741824
	Topic: poz-test02	Partition: 0	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2
	Topic: poz-test02	Partition: 1	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
```

파티션 개수를 2로 두고 생성한 후 살펴보면 위와 같이 나온다. 해당 토픽의 파티션 개수만 따지게되면 6개가 되는 것이다.

```sh
sh kafka-console-producer.sh --bootstrap-server {server-name} --topic {topic.name}

> test message1
```

콘솔 프로듀서 쉘을 이용하여 해당 토픽에 메세지를 집어 넣은 후  

```sh
/usr/local/kafka/bin/kafka-dump-log.sh --print-data-log --files /data/kafka-logs/{topic-name}-0/00000000000000000000.log

Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1709642835454 size: 81 magic: 2 compresscodec: NONE crc: 3432710041 isvalid: true
| offset: 0 CreateTime: 1709642835454 keysize: -1 valuesize: 13 sequence: -1 headerKeys: [] payload: test message1
```

카프카 덤프 로그 쉘을 이용하여 로그 정보를 살펴보았다.  
시작 오프셋 위치가 0이고 메세지 카운트가 1이고 보낸 메세지가 'test message1' 인 것을 확인할 수가 있다.  

<br/>

프로듀서는 모든 리플리케이션에 메세지를 보내는 것이 아니라 리더에게만 메세지를 전송한다.  
또한 컨슈머도 오직 리더로부터 메세지를 가져온다.  

팔로워는 리더에 문제가 발생하거나 이슈가 있을 경우를 대비해 언제든지 새로운 리더가 될 준비를 한다.  
지속적으로 파티션의 리더가 새로운 메세지를 받았는지 확인하고, 새로운 메세지가 있다면 해당 메세지를 리더로부터 복제한다.  

위에서 describe 옵션으로 토픽을 조회했을 때 Isr(InSyncReplicas) 이라는 값이 나오는데, 이곳에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있다. 만약 리더의 데이터에 뒤쳐진다면 Isr 그룹에서 추방한다.  

<br/>

레플리케이션이 있을 때 팔로워가 리더한테서 데이터를 제대로 가져오기 전에 리더가 데이터를 가져왔다고 바로 커밋을 해버리면 문제가 생긴다. 예를들면 리더한테 message1, message2가 들어가있고 컨슈머 A가 컨슘을해서 2개의 메세지를 가져간 후 팔로워가 리더의 데이터를 가져가기 전에 리더가 죽어서 다른 팔로워가 리더가 되고 컨슈머 B가 컨슘을 하게되면 1개의 메세지만 가져가게 된다.  

그렇기 때문에 모든 팔로워가 해당 데이터를 가져간 것이 확실해졌을 때 커밋을 하게 된다.  

마지막 커밋 오프셋 위치를 **하이워터마크**라고 부른다. 

```sh
cat /data/kafka-logs/replication-offset-checkpoint

{topic-name} 0 1
```

커밋된 오프셋 위치를 확인하기 위해 위와 같이 확인해보았다.  
첫 번째는 써놨듯이 토픽명이고 두번째는파티션 번호, 세번째는 커밋된 오프셋 번호를 뜻한다.  
이 상황에서 토픽에 메세지를 추가로 넣게 되면 ``{topic-name} 0 2`` 이렇게 변경된다.  

<br/>

리더와 팔로워의 레플리케이션 동작의 순서는 대략 다음과 같다.  

1. 프로듀서가 리더에 메세지를 넣는다
2. 팔로워들이 리더한테 새로운 메세지가 생겼는지 확인을 한다, 이때 리더는 팔로워가 몇 번의 오프셋에 해당하는 메세지를 리플리케이션하기 위해 요청을 보냈다는 사실을 알고 있다. 하지만 리더는 팔로워들이 이 레플리케이션 동작이 성공했는지 실패했는지 여부를 알지 못한다. 카프카는 이렇게 일반적으로 ACK를 이용하는 방식을 이용하여 성능을 높였다.
3. 프로듀서가 리더에 새로운 메세지를 넣는다
4. 팔로워가 리더한테 해당 메세지에 대한 요청을 한다. 이때 리더는 팔로워들이 요청하는 오프셋을 보고 1번에 해당하는 오프셋의 레플리케이션 동작이 성공했음을 인지하고 해당 오프셋에 대해 커밋 표시를 한 후 하이워터마크를 증가시킨다. 만약 팔로워가 레플리케이션 동작에 실패했다면 3번 절차에 들어온 메세지에 대한 요청이 아닌 1에 대한 메세지를 요청할 것이다.

이런 식으로 카프카는 리더가 푸시하는 방법이 아니라 풀하는 방식으로 동작하여, 리더의 부하를 줄여준다.  

<br/>

카프카의 파티션들이 복구 동작을 할 때 **리더에포크(Leader Epoch)**라는 것을 이용한다.  
리더에포크는 컨트롤러에 의해 관리되는 32비트의 숫자로 표현된다. 리더에포크 정보는 레플리케이션 프로토콜에 의해 전파되고, 새로운 리더가 변경된 후 변경된 리더에 대한 정보는 팔로워에게 전달된다. 리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용된다.  

만약 아래와 같은 경우로 복제 문제가 일어난다면, 리더에포크가 없다면 문제가 생긴다.

1. 리더가 message1를 받고 0번 오프셋에 저장함
2. 팔로워가 리더한테 메세지를 가져감
3. 리더가 하이워터마크를 1로 올림
4. 리더가 message2를 새로 받고 1번 오프셋에 저장함
5. 팔로워가 리더한테 메세지를 가져감
6. 리더가 팔로워가 요청한 오프셋 위치를 보고 하이워터마크를 1로 올림
7. 팔로워들은 message2를 레플리케이션함
8. 팔로워가 리더한테 오프셋 2위치에 대한 요청을 보냄(리더는 아직 메세지 안받아서 해당 위치에 데이터가 없음), 이떄 리더는 하이워터마크를 2로 올림
9. 팔로워가 리더한테 하이워터마크 변경에 대한 응답을 받지 못한 상태에서 팔로워가 다운

참고로 리더는 팔로워가 요청한 오프셋의 위치에 대한 메세지와 현재 하이워터마크에 대한 정보를 같이 전달한다.  



팔로워가 복구가 되었을 때 자신의 워터마크보다 높은 메세지들을 삭제하는데 이때 message2가 없어지게 된다. 그리고 이 순간 리더가 죽으면 메세지가 유실되게 되는 것이다.  

리더에포크를 사용하면 자신의 워터마크보다 높은 메세지를 삭제하지 않고 팔로워일 때와 리더일 떄의 하이워터마크를 둘 다 보존하여 복구를 진행하게 된다. 대충 이런 과정으로 데이터 문제 없이 복구가 완료된다.  

