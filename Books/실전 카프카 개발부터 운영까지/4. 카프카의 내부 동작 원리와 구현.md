# 카프카의 내부 동작 원리와 구현

## 카프카 레플리케이션

토픽 생성 시 레플리케이션을 사용하기 위해서는 ``replication factor`` 옵션을 설정해야 한다.  

```sh
sh kafka-topics.sh --bootstrap-server {server-name} --create --topic {topic-name} --partitions 1 --replication-factor 3
```

server를 작성할 때에는 카프카 클러스터에 연결하기 위한 초기 연결점이기 때문에 모든 브로커들을 작성할 필요는 없다.  

```sh
sh kafka-topics.sh --bootstrap-server {server-name} --topic {topic-name} --describe

Topic: peter-test01	PartitionCount: 1	ReplicationFactor: 3	Configs: segment.bytes=1073741824
	Topic: peter-test01	Partition: 0	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2	
```

생성 후 describe 옵션을 이용하여 토픽을 살펴보면 위와 같이 결과가 나온다.  
파티션 개수와 레플리케이션 개수가 나오고, 어떤 브로커가 리더인지 나온다.  

```sh
sh kafka-topics.sh --bootstrap-server {server-name} --topic {topic-name} --describe

Topic: poz-test02	PartitionCount: 2	ReplicationFactor: 3	Configs: segment.bytes=1073741824
	Topic: poz-test02	Partition: 0	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2
	Topic: poz-test02	Partition: 1	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
```

파티션 개수를 2로 두고 생성한 후 살펴보면 위와 같이 나온다. 해당 토픽의 파티션 개수만 따지게되면 6개가 되는 것이다.

```sh
sh kafka-console-producer.sh --bootstrap-server {server-name} --topic {topic.name}

> test message1
```

콘솔 프로듀서 쉘을 이용하여 해당 토픽에 메세지를 집어 넣은 후  

```sh
/usr/local/kafka/bin/kafka-dump-log.sh --print-data-log --files /data/kafka-logs/{topic-name}-0/00000000000000000000.log

Dumping /data/kafka-logs/peter-test01-0/00000000000000000000.log
Starting offset: 0
baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1709642835454 size: 81 magic: 2 compresscodec: NONE crc: 3432710041 isvalid: true
| offset: 0 CreateTime: 1709642835454 keysize: -1 valuesize: 13 sequence: -1 headerKeys: [] payload: test message1
```

카프카 덤프 로그 쉘을 이용하여 로그 정보를 살펴보았다.  
시작 오프셋 위치가 0이고 메세지 카운트가 1이고 보낸 메세지가 'test message1' 인 것을 확인할 수가 있다.  

### 리더와 팔로워

프로듀서는 모든 리플리케이션에 메세지를 보내는 것이 아니라 리더에게만 메세지를 전송한다.  
또한 컨슈머도 오직 리더로부터 메세지를 가져온다.  

팔로워는 리더에 문제가 발생하거나 이슈가 있을 경우를 대비해 언제든지 새로운 리더가 될 준비를 한다.  
지속적으로 파티션의 리더가 새로운 메세지를 받았는지 확인하고, 새로운 메세지가 있다면 해당 메세지를 리더로부터 복제한다.  

위에서 describe 옵션으로 토픽을 조회했을 때 Isr(InSyncReplicas) 이라는 값이 나오는데, 이곳에 속한 팔로워들만이 새로운 리더의 자격을 가질 수 있다. 만약 리더의 데이터에 뒤쳐진다면 Isr 그룹에서 추방한다.  

### 복제 유지와 커밋

레플리케이션이 있을 때 팔로워가 리더한테서 데이터를 제대로 가져오기 전에 리더가 데이터를 가져왔다고 바로 커밋을 해버리면 문제가 생긴다. 예를들면 리더한테 message1, message2가 들어가있고 컨슈머 A가 컨슘을해서 2개의 메세지를 가져간 후 팔로워가 리더의 데이터를 가져가기 전에 리더가 죽어서 다른 팔로워가 리더가 되고 컨슈머 B가 컨슘을 하게되면 1개의 메세지만 가져가게 된다.  

그렇기 때문에 모든 팔로워가 해당 데이터를 가져간 것이 확실해졌을 때 커밋을 하게 된다.  

마지막 커밋 오프셋 위치를 **하이워터마크**라고 부른다. 

```sh
cat /data/kafka-logs/replication-offset-checkpoint

{topic-name} 0 1
```

커밋된 오프셋 위치를 확인하기 위해 위와 같이 확인해보았다.  
첫 번째는 써놨듯이 토픽명이고 두번째는파티션 번호, 세번째는 커밋된 오프셋 번호를 뜻한다.  
이 상황에서 토픽에 메세지를 추가로 넣게 되면 ``{topic-name} 0 2`` 이렇게 변경된다.  

### 리더와 팔로워의 단계별 레플리케이션 동작

리더와 팔로워의 레플리케이션 동작의 순서는 대략 다음과 같다.  

1. 프로듀서가 리더에 메세지를 넣는다
2. 팔로워들이 리더한테 새로운 메세지가 생겼는지 확인을 한다, 이때 리더는 팔로워가 몇 번의 오프셋에 해당하는 메세지를 리플리케이션하기 위해 요청을 보냈다는 사실을 알고 있다. 하지만 리더는 팔로워들이 이 레플리케이션 동작이 성공했는지 실패했는지 여부를 알지 못한다. 카프카는 이렇게 일반적으로 ACK를 이용하는 방식을 이용하여 성능을 높였다.
3. 프로듀서가 리더에 새로운 메세지를 넣는다
4. 팔로워가 리더한테 해당 메세지에 대한 요청을 한다. 이때 리더는 팔로워들이 요청하는 오프셋을 보고 1번에 해당하는 오프셋의 레플리케이션 동작이 성공했음을 인지하고 해당 오프셋에 대해 커밋 표시를 한 후 하이워터마크를 증가시킨다. 만약 팔로워가 레플리케이션 동작에 실패했다면 3번 절차에 들어온 메세지에 대한 요청이 아닌 1에 대한 메세지를 요청할 것이다.

이런 식으로 카프카는 리더가 푸시하는 방법이 아니라 풀하는 방식으로 동작하여, 리더의 부하를 줄여준다.  

### 리더에포크와 복구

카프카의 파티션들이 복구 동작을 할 때 **리더에포크(Leader Epoch)** 라는 것을 이용한다.  
리더에포크는 컨트롤러에 의해 관리되는 32비트의 숫자로 표현된다. 리더에포크 정보는 레플리케이션 프로토콜에 의해 전파되고, 새로운 리더가 변경된 후 변경된 리더에 대한 정보는 팔로워에게 전달된다. 리더에포크는 복구 동작 시 하이워터마크를 대체하는 수단으로도 활용된다.  

만약 아래와 같은 경우로 복제 문제가 일어난다면, 리더에포크가 없다면 문제가 생긴다.

1. 리더가 message1를 받고 0번 오프셋에 저장함
2. 팔로워가 리더한테 메세지를 가져감
3. 리더가 하이워터마크를 1로 올림
4. 리더가 message2를 새로 받고 1번 오프셋에 저장함
5. 팔로워가 리더한테 메세지를 가져감
6. 리더가 팔로워가 요청한 오프셋 위치를 보고 하이워터마크를 1로 올림
7. 팔로워들은 message2를 레플리케이션함
8. 팔로워가 리더한테 오프셋 2위치에 대한 요청을 보냄(리더는 아직 메세지 안받아서 해당 위치에 데이터가 없음), 이떄 리더는 하이워터마크를 2로 올림
9. 팔로워가 리더한테 하이워터마크 변경에 대한 응답을 받지 못한 상태에서 팔로워가 다운

참고로 리더는 팔로워가 요청한 오프셋의 위치에 대한 메세지와 현재 하이워터마크에 대한 정보를 같이 전달한다.  



팔로워가 복구가 되었을 때 자신의 워터마크보다 높은 메세지들을 삭제하는데 이때 message2가 없어지게 된다. 그리고 이 순간 리더가 죽으면 메세지가 유실되게 되는 것이다.  

리더에포크를 사용하면 자신의 워터마크보다 높은 메세지를 삭제하지 않고 팔로워일 때와 리더일 떄의 하이워터마크를 둘 다 보존하여 복구를 진행하게 된다. 대충 이런 과정으로 데이터 문제 없이 복구가 완료된다.  

<br/>

## 컨트롤러

카프카 클러스터 중 하나의 브로커가 리더 선출을 맡는 컨트롤러의 역할을 하게 되며, 파티션의 ISR 리스트 중에서 리더를 선출한다. 리더를 선출하기 위한 ISR 리스트 정보는 가용성 보장을 위해 주키퍼에 저장되어 있다. 컨트롤러는 브로커가 실패하는 것을 주시하고 있으며, 브로커의 실패가 감지되면 즉시 ISR 리스트 중 하나를 새로운 파티션 리더로 선출한다. 그러고 나서 새로운 리더의 정보를 주키퍼에 기록하고, 변경된 정보를 모든 브로커에게 전달한다.  

파티션의 리더가 다운되었다는 것은 프로듀서나 컨슈머가 해당 파티션으로 읽기나 쓰기가 불가능하다는 것이고 클라이언트에 설정되어 있는 재시도 숫자만큼 재시도를 하게된다.  

파티션이 1개이고 레플리케이션 팩터가 2개일 때(1,3번 브로커), 예기치 않은 장애로 인한 리더 선출 과정은 아래와 같다.  

1. 파티션 0번의 리더가 있는 브로커 1번이 예기치 않게 다운
2. 주키퍼는 1번 브로커와의 연결이 끊어진 후, 0번 파티션의 ISR에서 변화가 생겼음을 감지
3. 컨트롤러는 주키퍼 워치를 통해 0번 파티션에 변화가 생긴 것을 감지하고, 해당 파티션 ISR 중 3번을 새로운 리더로 선출
4. 컨트롤러는 0번 파티션의 새로운 리더가 3이라는 정보를 주키퍼에 기록
5. 갱신된 정보는 현재 활성화 상태인 모든 브로커에게 전파됨

<Br/>

제어된 종료 과정은 아래와 같다.

1. 관리자가 브로커 종료 명령어를 실행하고, SIG_TERM 신호가 브로커에게 전달
2. SIG_TERM 신호를 받은 브로커는 컨트롤러에게 알림
3. 컨트롤러는 리더 선출 작업을 진행하고, 해당 정보를 주키퍼에 기록
4. 컨트롤러는 새로운 리더 정보를 다른 브로커들에게 전송
5. 컨트롤러는 종료 요청을 보낸 브로커에게 정상 종료한다는 응답을 보냄
6. 응답을 받은 브로커는 캐시에 있는 내용을 디스크에 저장하고 종료

제어된 종료는 브로커가 종료되기 전에 리더 선출 작업을 진행하기 때문에 다운타임이 아예 없다고 할 수는 없지만 최소화할 수 있다.  

<Br/>

## 로그(로그 세그먼트)

메세지는 정해진 형식에 맞추어 순차적으로 로그 세그먼트 파일에 저장된다. 이때 메세지만 저장되는 것이 아니라 메세지의 키, 밸류, 오프셋, 메세지 크기 같은 정보가 함께 저장되고, 로그 세그먼트 파일들은 브로커의 로컬 디스크에 보관된다.  

기본적으로 파일 관리의 용이함을 위해 1GB를 최대 크기로 가지고 있다. 1GB보다 커지는 경우에는 롤링전략을 적용한다. 로그 세그먼트의 크기가 1GB에 도달하면 해당 세그먼트 파일을 클로즈하고, 새로운 로그 세그먼트를 생성하는 방식으로 진행한다.  

이 파일이 무한히 늘어날 경우를 대비해서 관리 계획을 수립해둬야 한다. 방법으로는 크게 삭제와 컴팩션으로 구분할 수 있다.  

### 로그 세그먼트 삭제

로그 세그먼트 삭제 옵션은 server.properties에서 ``log.cleanup.policy``가 delete로 명시되어 있어야 한다. 하지만 이 값은 기본값이다.  

```sh
sh kafka-configs.sh --bootstrap-server {server-name} --topic {topic-name} --add-config retention.ms=0 --alter
```

```sh
sh kafka-topics.sh --bootstrap-server {server-name} --topic {topic-name} --describe

Topic: peter-test01	PartitionCount: 1	ReplicationFactor: 3	Configs: segment.bytes=1073741824,retention.ms=0
	Topic: peter-test01	Partition: 0	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2	
```

``retention.ms`` 설정을 추가하고 세그먼트가 저장되어 있는 ``/data/kafka-logs`` 디렉터리로 가보자.  

로그 세그먼트의 삭제 작업은 일정 주기를 가지고 체크하는데, 카프카의 기본값은 5분 주기이므로 5분 간격으로 로그 세그먼트 파일을 체크하면서 삭제 작업을 수행한다. 따라서 명령어 실행 뒤 즉시 로그 세그먼트 파일이 삭제되는 것이 아니라 약 5분 후에 삭제 작업이 일어난다.  

보면 ``0000...0000.log`` 이런 로그 파일이 삭제되고 ``0000...0001.log``이렇게 값이 바뀐 파일들이 추가된 것을 확인할 수 있다. 카프카에서는 로그 세그먼트 파일명이 생성되는 규칙이 있따. 로그 세그먼트 파일을 생성할 때 오프셋 시작 번호를 이용해 파일 이름을 생성하는 규칙을 따른다. 만약 해당 토픽의 파티션에 메세지가 2개 쌓여있는 상황에서 새로운 파일이 생성이 되었다면 ``0000...0002.log`` 와 같은 파일명이 생성되었을 것이다.  

``retention.ms``의 기본 설정값은 7일이다. 그리고 ``retention.bytes``라는 옵션을 이용해 지정된 크기를 기준으로도 로그 세그먼트를 삭제할 수도 있다.  

### 로그 세그먼트 컴팩션

컴팩션은 카프카에서 제공하는 로그 세그먼트 관리 정책 중 하나로, 로그를 삭제하지 않고 컴팩션하여 보관할 수 있다. 로그 컴팩션은 기본적으로 로컬 디스크에 저장되어 있는 세그먼트를 대상으로 실행되는데, 현재 활성화된 세그먼트는 제외하고 나머지 세그먼트들을 대상으로 컴팩션이 실행된다.  

컴팩션할지라도 카프카의 로컬 디스크에 로그를 무기한 보관한다면, 용량의 한계에 도달할 것이다. 따라서 카프카에서는 단순하게 메세지를 컴팩션하여 보관하기보다는 좀 더 효율적인 방법으로 컴팩션한다. 카프카에서는 로그 세그먼트를 컴팩션하면 메세지의 키값을 기준으로 마지막의 데이터만 보관하게 된다.  

이 로그 컴팩션 기능을 이용하는 대표적인 예제는 바로 카프카의 ``__consumer_offset`` 토픽이다. 이 토픽은 카프카의 내부 토픽으로, 컨슈머 그룹의 정보를 저장하는 토픽이다. 각 컨슈머 그룹의 중요한 정보는 해당 컨슈머 그룹이 어디까지 읽었는지를 나타내는 오프셋 위치 정보인데, ``__consumer_offset``에 키(컨슈머 그룹명, 토픽명)와 밸류(오프셋 커밋 정보) 형태로 메세지가 저장된다.  

만약 컨슈머그룹 1이 토픽 1에 오프셋 0의 위치에 커밋을 하고 이후에 1의 위치에 커밋을 하게되면 최종적으로 1이라는 정보만 갖고 있어도 될 것이다. 그리고 로그 컴팩션을 하게되면 컨슈머그룹 1, 토픽 1의 키에 오프셋의 값인 1의 밸류를 가진 값만 남게된다.  

또 다른 예로 구매 상태 정보에 대한 메세지이고 주문 완료 -> 배송 준비 -> 배송 중 -> 배송 완료 이런 정보들이 있을 때 가장 마지막 값만 갖고 있어도 될 것이다. 이런 경우에도 사용이 가능하다.  

로그 컴팩션의 장점은 빠른 장애 복구이다. 메세지의 키를 기준으로 최신의 상태만 복구하면 되기 때문이다. 전체 로그를 복구할 때보다 복구 시간을 줄일 수 있다는 장점이 있다. 그렇다고 모든 토픽에 로그 컴팩션을 적용하면 안되고 알맞는 상황에서만 사용해야한다. 카프카에서 로그 컴팩션 작업이 실행되는 동안 브로커의 과도한 입출력 부하가 발생할 수 있으니 브로커의 리소스 모니터링을 병행하여 컴팩션을 사용하기를 권장한다.  

로그 컴팩션 관련 옵션은 아래와 같다.

|             옵션 이름             | 옵션값  |         적용 범위         |                             설명                             |
| :-------------------------------: | ------- | :-----------------------: | :----------------------------------------------------------: |
|          cleanup.policy           | compact |   토픽의 옵션으로 적용    |     토픽 레벨에서 로그 컴팩션을 설정할 때 적용되는 옵션      |
|        log.cleanup.policy         | compact | 브로커의 설정 파일에 적용 |    브로커 레벨에서 로그 컴팩션을 설정할 때 적용하는 옵션     |
| log.cleaner.min.compaction.lag.ms | 0       | 브로커의 설정 파일에 적용 | 메세지가 기록된 후 컴팩션하기 전 경과되어야 할 최소 시간을 지정, 이 옵션을 설정하지 않으면, 마지막 세그먼트를 제외하고 모든 세그먼트를 컴팩션할 수 있다. |
| log.cleaner.max.compaction.lag.ms |         | 브로커의 설정 파일에 적용 | 메세지가 기록된 후 컴팩션하기 전 경과되어야 할 최대 시간을 지정 |
|  log.cleaner.min.cleanable.ratio  | 0.5     | 브로커의 설정 파일에 적용 | 로그에서 압축이 되지 않은 부분을 더티라고 표현한다. 전체 로그 대비 더티의 비율이 50ㅖ%가 넘으면 로그 컴팩션이 실행된다. |

``cleanup.policy``는 기본값이 설정되어있지 않고 기본적으로 브로커의 ``log.cleanup.policy`` 설정 값을 따르게 된다.  
만약 ``cleanup.policy``가 compact으로 되어있고 ``log.cleanup.policy``가 delete 이라면 전자를 우선적으로 따르게된다.  

delete가 아니라 compact이 설정되어있다면 ``retention.ms`` 값이 있어도 세그먼트가 삭제되지 않는다. 삭제가 아니라 컴팩션으로 정책이 정해져있기 때문이다.  

---

