# 스키마 레지스트리

RDB 처럼 카프카에도 스키미가 존재한다. 한 토픽을 여러 컨슈머 그룹이 바라보고 있는 상황에서 옳지 않은 데이터 형식이 들어오게되면 문제가 발생할 것이다. 이를 위해 스키마를 정의해두어 데이터 트러블 슈팅 감소, 용이한 데이터 포맷 확인, 데이터 스키마 관련 커뮤니케이션 감소 등 여러 이점을 얻을 수 있다.  

<br/>

## 카프카와 스키마 레지스트리

### 스키마 레지스트리 개요

카프카에서 스키마를 활용하는 방법은 스키마 레지스트리라는 별도의 애플리케이션을 이용하는 것이다. 

<img width="867" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/df063b4a-1808-44ed-9b67-525b02ef727c">

위의 그림과 같이 스키마 레지스트리는 카프카와 별도로 구성된 독립적 애플리케이션으로, 카프카로 메세지를 전송하는 프로듀서와 직접 통신하며 카프카로부터 메세지를 꺼내오는 컨슈머와도 직접 통신한다.  

프로듀서는 스키마 레지스트리에 스키마를 등록하고, 스키마 레지스트리는 프로듀서에 의해 등록된 스키마 정보를 카프카의 내부 토픽에 저장한다. 프로듀서는 스키마 레지스트리에 등록된 스키마의 ID와 메세지를 카프카로 전송하고, 컨슈머는 스키마 ID를 스키마 레지스트리로부터 읽어온 후 프로듀서가 전송한 스키마 ID와 메세지를 조합해 읽을 수 있다.  

스키마 레지스트리를 이용하기 위해서는 레지스트리가 지원하는 데이터 포맷을 사용해야 하는데, 가장 대표적인 포맷이 에이브로다.  

### 스키마 레지스트리의 에이브로 지원

에이브로(Avro, hey bro 아님 ㅎ)는 시스템, 프로그래밍 언어, 프로세싱 프레임워크 사이에서 데이터 교환을 도와주는 오픈소스 직렬화 시스템으로서, 빠른 바이너리 데이터 포맷을 지원하는 JSON 형태의 스키마를 정의할 수 있는 매우 간결한 데이터 포맷이다.  

레지스트리는 JSON도 지원하지만 confluent에서는 아래와 같은 이유로 에이브로를 권장하고 있다.

* 에이브로는 JSON과 매핑된다.
* 에이브로는 매우 간결한 데이터 포맷이다.
* JSON은 메세지마다 필드 네임들이 포함되어 전송되므로 효율이 떨어진다.
* 에이브로는 바이너리 형태이므로 매우 빠르다.

```avro
{"namespace": "student.avro",
 "type": "record",
 "doc": "This is an example of Avro.",
 "name": "Student",
 "fields": [
     {"name": "name", "type": "string", "doc": "Name of the student"},
     {"name": "class", "type": "int", "doc": "Class of the student"}
 ]
}
```

* namespace: 이름을 식별하는 문자열  
* type: 에이브로는 record, enums, arrays, maps 등을 지원하며, 여기서는 record 타입
* doc: 사용자들에게 이 스키마 정의 대한 설명 제공
* name: 레코드의 이름을 나타내는 문자열, 필숫값
* fields: JSON 배열로서, 필드들의 리스트를 뜻함
* name: 필드의 이름

에이브로는 JSON과 달리 데이터 필드마다 데이터 타입을 정의할 수 있고, doc을 이용해 의미도 전달 가능하다.  

### 설치

```sh
sudo wget http://packages.confluent.io/archive/6.1/confluent-community-6.1.0.tar.gz -O /opt/confluent-community-6.1.0.tar.gz
```

다운을 받고 압축해제한 후 ``/etc/schema-registry/schema-registry.properties`` 파일을 수정한다.  

* listeners: 스키마 레지스트리에서 사용할 TCP 포트 지정
* kafkastore.bootstrap.servers: 스키마의 버전 히스토리 및 관련 데이터를 저장할 카프카 주소
* kafkastore.topic: 스키마의 버전 히스토리 및 관련 데이터 저장 토픽의 이름
* schema.compatability: 스키마 호환성 레벨

내가 브로커가 30개고 ``kafkastore.bootstrap.servers``에 1-10번에 해당하는 브로커만 입력한 상황에서, 클라이언트의 컨슈머가 컨슘하는 토픽 파티션의 위치가 11-30번에 해당하는 브로커 서버라면 스키마 체크를 하지 못하는 것인가 궁금했는데, 구글 제미나이에 물어보니 컨슈머는 ``kafkastore.bootstrap.servers`` 설정에 지정된 서버 중 하나에 연결하고, 연결된 서버에 스키마 ID를 요청하고 서버는 스키마 레지스트리에 연결해서 스키마 ID에 해당하는 스키마를 가져오고 서버는 스키마를 컨슈머에게 전송하고 컨슈머는 스키마를 사용해서 메세지를 역직렬화 한다고 한다. 말이 좀 복잡하긴 하지만, 스키마 체크는 제대로 된다는 뜻. 고가용성을 위해 많은 브로커를 등록하면 좋을 것 같다고 생각한다.  

그리고 ``kafkastore.topic`` 토픽이 스키마 레지스트리의 저장소로 활용되고 스키마의 제목, 버전, ID 등이 저장되는데 이러한 스키마 관리 목적으로 사용되는 메세지들은 순서가 중요하기 때문에 해당 토픽의 파티션 수는 항상 1이다. 그리고 cleanup.policy가 compact이다.  

```service
[Unit]
Description=schema registry
After=network.target

[Service]
Type=simple
ExecStart=/usr/local/confluent/bin/schema-registry-start /usr/local/confluent/etc/schema-registry/schema-registry.properties
Restart=always

[Install]
WantedBy=multi-user.target
```

service 파일을 만들고 시작해준다.  

<Br/>

## 스키마 레지스트리 실습

<img width="927" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/1b70faf5-d5b6-4f00-aad2-0b5f0e6a593b">

1. ``io.confluent.kafka.serializers.KafkaAvroSerializer`` 를 사용해 스키마 레지스트리의 스키마가 유효한지 여부를 확인한다. 만약 스키마가 확인되지 않으면, 에이브로 프로듀서는 스키마를 등록하고 캐시한다.  
2. 스키마 레지스트리는 현 스키마가 저장소에 저장된 스키마와 동일한 것인지, 진화한 스키마인지를 확인한다. 스키마 레지스트리 자체적으로 각 스키마에 대해 고유 ID를 할당하게 된다. 이 ID는 순차적으로 1씩 증가하지만, 반드시 연속적이지는 않다. 스키마에 문제가 없다면 스키마 레지스트리는 프로듀서에게 고유 ID를 응답한다.
3. 프로듀서는 레지스트리로부터 받은 스키마 ID를 참고해 메세지를 카프카로 전송한다. 이때 프로듀서는 스키마의 전체 내용이 아닌 오로지 메세지와 스키마 ID만 보낸다. 
4. 에이브로 컨슈머는 스키마 ID로 ``io.confluent.kafka.serializers.KafkaAvroDeserializer`` 를 사용해 카프카의 토픽에 저장된 메세지를 읽는다. 이때 컨슈머가 스키마 ID를 갖고 있지 않다면 스키마 레지스트리로부터 가져온다. 

이렇게 프로듀서와 컨슈머는 직접 스키마를 주고 받지 않고 레지스트리와 통신하면서 스키마의 정보를 주고받을 수 있다.  

<br/>

gradle은 아래와 같이 진행했다.  

```gradle
repositories {
    mavenCentral()
    maven { url "https://packages.confluent.io/maven/" }
}

dependencies {
    implementation 'org.springframework.boot:spring-boot-starter'
    implementation "org.apache.kafka:kafka_2.13:${version}"
    implementation 'org.springframework.kafka:spring-kafka'
    implementation "org.apache.avro:avro:${version}"
    implementation "io.confluent:kafka-streams-avro-serde:${version}"
    implementation 'org.jboss.resteasy:resteasy-jaxrs:3.15.6.Final'

    testImplementation 'org.springframework.boot:spring-boot-starter-test'
}
```

confluent maven을 추가해야 kafka-stream-avro-serde를 받아올 수가 있었다.  
jboss 어쩌고는 uriBuilder 가 없다고 떠서 일단 넣었다.  

프로듀서와 컨슈머는 아래와 같이 코드를 작성했다.  

```java
public class Machine {

    public void doIt() {
        final String bootstrapServers = "";
        Properties configs = new Properties();
        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                    StringSerializer.class.getName());
        configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                    KafkaAvroSerializer.class.getName());
        configs.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG,
                    "");

        Sample sample = new Sample("kang", 22, 102, 400, "sub", "vegetable");

        KafkaProducer<String, Sample> producer = new KafkaProducer(configs);
        ProducerRecord<String, Sample> record = new ProducerRecord("produce-avro-test", "13", sample);
        producer.send(record);
        producer.flush();
        producer.close();
    }

    public void consumeIt() {
        final String bootstrapServers = "";
        Properties configs = new Properties();
        configs.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        configs.put(ConsumerConfig.GROUP_ID_CONFIG, "bepoz");
        configs.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
                    StringDeserializer.class.getName());
        configs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
                    KafkaAvroDeserializer.class.getName());
        configs.put(KafkaAvroSerializerConfig.SCHEMA_REGISTRY_URL_CONFIG,
                    "");

        KafkaConsumer<String, Sample> consumer = new KafkaConsumer(configs);
        consumer.subscribe(Collections.singleton("produce-avro-test"));
        while (true) {
            ConsumerRecords<String, Sample> records = consumer.poll(Duration.ofSeconds(1));
            for (ConsumerRecord<String, Sample> record : records) {
                System.out.println("##"+record.key());
                System.out.println("###"+record.value());
                System.out.println("####"+record);
            }
        }
    }
}
```

나는 처음에 스키마 등록을 따로 registry url로 api를 날려서 해줘야하는 줄 알았다. 그건 아니고 avro를 사용한다는 뜻은 곧 schema를 사용한다는 뜻이고, kafka에서는 스키마 관리를 위한 중앙 집중식 서비스로 schema registry를 주로 사용한다. 프로듀싱을 하면서 registry에 등록을 하게된다. 위의 코드를 보면 registry url 설정을 하는 코드라인을 확인할 수가 있다(debezium cdc를 이용하면 결국 avro 파일을 topic에 넣어주게 되고 이 과정에서 registry에 자동으로 등록이 되는 것이 이렇나 이유에서 였다). 

schema registry가 아니더라도 내장 스키마나 유사한 역할을 하는 사용자 정의 서비스를 구축할 수도 있다는데, 그냥 schema registry를 사용한다고 생각해도 무방할 것 같다.  

위의 코드에서 ``KafkaProducer``와 ``KafkaConsumer``를 사용했다. 만약 spring 환경이라면 굳이 이것들을 사용할 필요없이 ``KafkaTemplate``을 사용하는편이 더 편리하다(``KafkaTemplate``을 사용하면 번거롭게 ``ProducerRecord`` 같은 값을 사용하지 않아도 된다). 그리고 spring의 autoConfiguration을 이용해서 위의 코드처럼 설정을 직접 한 줄씩 치지 않아도 된다.  

avro 형식을 사용하기 위해 main 디렉터리 밑에 avro 디렉터리를 만들고 avsc 파일을 생성해주었다.  

```avro
{
  "namespace": "ksy.sample",
  "type": "record",
  "name": "Sample",
  "fields": [
    {
      "name": "name",
      "type": "string"
    },
    {
      "name": "age",
      "type": "int"
    }
  ]
}
```

그리고 이것을 java 파일로 만들기 위해서 ``id "com.github.davidmc24.gradle.plugin.avro" version "1.3.0"`` 플러그인을 build.gradle에 추가해주었다. 다른 플러그인으로도 java 파일로 변형이 가능하다. 그리고 ``generateAvroJava`` task를 진행하면 java 클래스로 만들어 준다. 아니면 그냥 ``new Schema.Parser()`` 을 이용하여 avsc 파일을 사용하는  방법도 있다. 대충 이 정도 내용이 사실 끝인데, 중요한 내용이 하나 더 있다. 스키마 변경이 일어났을 때다.  

스키마를 사용하는 이유 자체가 프로듀서 쪽에서 필드를 변경하거나 했을 때에 컨슈머 쪽에서 영향을 받지 않게끔 하기 위함인데,  
이런 호환성의 기준이 다 있다.  

기본 값은 ``BACKWARD``다. schema registry url에 ``/config``을 입력하면 호환성 타입을 확인할 수 있다.  

정확한 설명은 [공식 문서](https://docs.confluent.io/platform/current/schema-registry/fundamentals/schema-evolution.html#summary)를 확인하는 것이 제일 좋긴하다.  

<img width="1093" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/b3b28213-ccb1-486e-b268-27bf1cba70e3">

default 호환성 타입인 ``BACKWARD``를 설명하자면, 버전이 높은 스키마를 적용한 컨슈머가 그 버전보다 낮은 버전의 스키마가 적용된 프로듀서가 보낸 메세지를 읽을 수 있또록 허용하는 호환성을 의미한다. ``FORWARD``는 그 반대고, ``FULL``은 전부다.  

이것을 테스트하기 위해 컨슈머를 위한 프로젝트를 따로 만들어서 avsc 파일을 생성했다. 저기 공식 문서에서 ``BACKWARD``의  Changes allowed에 delete fields가 적혀져있길레 프로듀서 쪽의 avro 스키마에서 필드를 삭제하니 409 에러가 발생했다. 알고보니 컨슈머 쪽에서 먼저 필드를 지우는 경우를 의미한 거였다. 이 경우 프로듀서가 프로듀싱한 데이터에서 해당 필드는 무시된채 컨슘된다. 그리고 필드를 추가할 때에는 default 값을 지정해서 추가를 해야한다. 컨슈머 쪽에서 필드를 추가하고 어플리케이션을 다시 실행해도 schema registry에 해당 스키마의 version이 올라가지 않는 것을 확인할 수 있었다. 데이터가 실제로 프로듀싱되어 토픽에 들어가는 과정에서 버전이 올라가는 것 같다. 필드를 추가할 때 프로듀서 쪽에서 필드를 추가할 때 무조건 default 값을 추가해야만 정확히 실행이 된다. 굉장히 문서 설명이 애매한 것 같긴하지만... 아무튼 그러했다.  

그런데 default 값이야 뭐 null 값을 주거나 하는 형식으로 진행하면 되니깐 큰 문제는 되지 않을 것 같다. 어차피 해당 필드를 추가하고 진행한다고 해도 컨슈머 쪽 로직에서는 해당 필드를 가져다가 사용을 하지 않는 상태일 것이니 영향이 있기가 힘들다고 본다.

---

### REFERENCE

https://docs.confluent.io/platform/current/schema-registry/fundamentals/schema-evolution.html#summary

https://www.youtube.com/watch?v=vQ4mPepAM7Q

