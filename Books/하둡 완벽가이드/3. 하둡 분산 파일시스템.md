# 하둡 분산 파일시스템

네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템을 **분산 파일시스템**이라고 한다.  
분산 파일시스템은 네트워크 기반이므로 네트워크 프로그램의 복잡성을 모두 가지고 있다. 따라서 일반적인 디스크 파일시스템보다 훨씬 더 복잡하다.  

하둡은 HDFS(Hadoop Distributed FileSystem)라는 분산 파일시스템을 제공한다. 

<br/>

## HDFS 설계

HDFS는 범용 하드웨어로 구성된 클러스터에서 실행되고 스트리밍 방식의 데이터 접근 패턴으로 대용량 파일을 다룰 수 있도록 설계된 파일시스템이다.  

특징은 아래와 같다.  

* 매우 큰 파일
* 스트리밍 방식의 데이터 접근
* 범용 하드웨어

하지만 아래와 같은 경우에서는 적절하지 않다.

* 빠른 데이터 응답 시간
  * HDFS는 높은 데이터 처리량을 제공하기 위해 최적화되어 있고 이를 위해 응답 시간을 희생했다.
* 수많은 작은 파일
* 다중 라이터와 파일의 임의 수정
  * HDFS는 단일 라이터로 파일을 쓴다. 한 번 쓰고 끝나거나 파일의 끝에 덧붙이는 것은 가능하지만 파일에서 임의 위치에 있는 내용을 수정하는 것은 허용하지 않으며 다중 라이터도 지원하지 않는다.

<Br/>

## HDFS 개념

### 블록

물리적인 디스크는 블록 크기란 개념이 있고 블록 크기는 한 번에 읽고 쓸 수 있는 데이터의 최대량이다. 단일 디스크를 위한 파일 시스템은 디스크 블록 크기의 정배수인 파일시스템 블록 단위로 데이터를 다룬다.  

파일시스템 블록의 크기는 보통 수 킬로바이트고, 디스크 블록의 크기는 기본적으로 512바이트다. 사용자는 파일의 크기와 상관없이 파일을 읽고 쓸 수 있으며, 특정 파일 시스템에 구애받지도 않는다. 하지만 파일시스템의 블록 수준에서 파일시스템의 유지관리를 수행하는 df나 fsck와 같은 도구도 있다.  

HDFS도 블록의 개념을 가지고 있으며 기본적으로 128MB이다. HDFS의 파일은 블록 크기의 청크로 쪼개지고 각 청크는 독립적으로 저장된다. 블록 크기가 128MB이고 이것보다 작은 파일을 저장한다면 128MB의 디스크를 사용하는 것이 아니라 딱 해당 사이즈에 맞는 디스크만 사용한다.  

> HDFS 블록이 큰 이유는 탐색 비용을 최소화하기 위해서다. 블록이 매우 크면 블록의 시작점을 탐색하는 데 걸리는 시간을 줄일 수 있고 데이터를 전송하는 데 더 많은 시간을 할애할 수 있다.

분산 파일시스템에 블록 추상화의 개념을 도입하면,

1.  파일 하나의 크기가 단일 디스크의 용량보다 더 커질 수 있다.
2. 스토리지의 서브 시스템을 단순하게 만들 수 있다.
   1. 블록은 고정 크기고 저장에 필요한 디스크 용량만 계산하면 된다. 블록은 단지 저장된 데이터의 청크일 뿐이고 권한 정보와 같은 파일의 메타데이터는 블록과 함께 저장될 필요가 없으므로 별도의 시스템에서 다루도록 분리할 수 있다.
3. 내고장성(fault tolerance)과 가용성(availability)을 제공하는데 필요한 복제(replication)를 구현할 때 매우 적합하다.

<Br/>

### 네임노드와 데이터노드

HDFS 클러스터는 마스터-워커 패턴으로 동작하는 **네임노드**와 여러 개의 **데이터노드**로 구성되어 있다.  

네임노드는 파일시스템의 네임스페이스를 관리한다. 네임노드는 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 유지한다. 이 정보는 네임스페이스 이미지와 에디트 로그라는 두 종류의 파일로 로컬 디스크에 영속적으로 저장된다. 네임노드는 또한 파일에 속한 모든 블록이 어느 데이터노드에 있는지 파악하고 있다. 하지만 블록 위치 정보는 시스템이 시작할 때 모든 데이터노드로부터 받아서 재구성하기 때문에 디스크에 영속적으로 저장하지는 않는다.  

HDFS 클라이언트는 사용자를 대신해서 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근하기 때문에 사용자는 네임노드와 데이터노드에 관련된 함수를 몰라도 코드를 작성할 수 있다.  

<Br/>

데이터노드는 클라이언트나 네임노드의 요청이 있을 때 블록을 저장하고 탐색하며, 저장하고 있는 블록의 목록을 주기적으로 네임노드에 보고한다.  

네임노드가 없으면 파일시스템은 동작하지 않기 때문에 장애복구 기능은 필수적이다.  
하둡은 이를 위해 두 가지 메커니즘을 제공한다.  

1. 파일시스템의 메타데이터를 지속적인 상태로 보존하기 위해 파일로 백업하는 것이다. 네임노드가 다수의 파일시스템에 영구적인 상태를 저장하도록 하둡을 구성할 수 있다. 백업 작업은 동기화되고 원자적으로 실행된다. 주로 권장하는 방법은 로컬 디스크와 원격의 NFS 마운트 두 곳에 동시에 백업하는 것이다.
2. **보조 네임노드**를 운영하는 것이다. 주 역할은 에디트 로그가 너무 커지지 않도록 주기적으로 네임스페이스 이미지를 에디트 로그와 병합하여 새로운 네임스페이스 이미지를 만드는 것이다. 또한 주 네임노드에 장애가 발생할 것을 대비해서 네임스페이스 이미지의 복제본을 보관하는 역할도 맡는다. 하지만 주 네임노드의 네임스페이스 이미지는 약간의 시간차를 두고 보조 네임노드로 복제되기 때문에 주 네임노드에 장애가 발생하면 어느 정도의 데이터 손실은 불가피하다. 이럴 때 NFS에 저장된 주 네임노드의 메타데이터 파일을 보조 네임노드로 복사하여 새로 병합된 네임스페이스 이미지를 만들고 그것을 새로운 주 네임노드에 복사한 다음 실행하는 방법이 있다.

<br/>

### 블록 캐싱

빈번하게 접근되는 블록 파일은 **블록 캐시**라는 데이터노드의 메모리에 명시적으로 캐싱할 수 있다.  
기본적으로 블록은 하나의 데이터노드 메모리에만 캐싱되지만 파일 단위로 설정도 가능하며, 잡 스케줄러는 블록이 캐싱된 데이터노드에서 태스크가 실행되도록 할 수 있으며, 이러한 블록 캐시의 장점을 이용하면 읽기 성능을 높일 수 있다.  

사용자나 애플리케이션은 **캐시 풀**에 **캐시 지시자**를 추가하여 특정 파일을 캐싱하도록 명령할 수 있다.  

<Br/>

### HDFS 고가용성

위의 여러가지 방법들이 있었지만 여전히 네임노드는 **단일 고장점(Single Point Of Failure, SPOF)**이다.  
네임노드에 장애가 발생하면 맵리듀스 잡을 포함하여 모든 클라이언트가 파일을 읽거나 쓰거나 조회할 수 없게 된다.  

네임노드의 장애를 복구하기 위해 관리자는 파일시스템 메타데이터 복제본을 가진 새로운 네임노드를 구동하고 모든 데이터노드와 클라이언트에 새로운 네임노드를 사용하도록 알려주면 된다. 하지만 여러 절차로 인해 30분 이상 걸리는 경우도 있다.  

이 문제를 해결하기 위해 하둡 2.x 릴리즈부터 HDFS 고가용성(HA)을 지원한다.  
고가용성은 활성대기 상태로 설정된 한 쌍의 네임노드로 구현된다. 활성 네임노드에 장애가 발생하면 대기 네임노드가 그 역할을 이어받아 큰 중단없이 클라이언트의 요청을 처리한다. 이러한 방식을 지원하기 위해 HDFS의 구조를 일부 변경했다.

* 네임노드는 에디트 로그를 공유하기 위해 고가용성 공유 스토리지를 반드시 사용해야 한다. 

  대기 네임노드가 활성화되면 먼저 기존 활성 네임노드의 상태를 동기화하기 위해 공유 에디트 로그를 읽고, 이어서 활성 네임노드에 새로 추가된 항목도 마저 읽는다.

* 데이터노드는 블록 리포트를 두 개의 네임노드에 보내야 한다. 블록 매핑 정보는 디스크가 아닌 네임노드의 메모리에 보관되기 때문이다.

* 클라이언트는 네임노드 장애를 사용자에게 투명한 방식으로 처리할 수 있도록 구성해야 한다.

* 대기 네임노드는 보조 네임노드의 역할을 포함하고 있으며, 활성 네임노드 네임스페이스의 체크포인트 작업을 추가적으로 수행한다.

활성 네임노드에 장애가 발생하면 대기 네임노드는 매우 빠르게(수십 초 이내) 기존 네임노드를 대체할 수 있다. 활성과 대기 네임노드는 모두 최신 에디트 로그와 실시간으로 갱신되는 블록 매핑 정보를 메모리에 유지하고 있기 때문이다. 하지만 실제로 장애 복구 시간을 보면 1분 정도 걸리는데, 시스템이 활성 네임노드에 장애가 발생했다고 판단하는 것은 매우 신중해야 하기 때문이다.  

#### 장애복구와 펜싱

대기 네임노드를 활성화시키는 전환 작업은 **장애복구 컨트롤러**라는 새로운 객체로 관리된다. 다양한 방법으로 장애복구 컨트롤러를 구현할 수 있지만 기본 구현체는 단 하나의 네임노드만 활성 상태에 있는 것을 보장하기 위해 주키퍼를 이용한다. 각 네임노드는 경량의 장애복구 컨트롤러 프로세스로 네임노드의 장애를 감시하고 네임노드에 장애가 발생하면 장애복구를 지시한다.  

관리자가 수동으로 유지관리를 위해 초기화하는 것을 **우아한 장애복구**라고 하는데 그렇지 못하는 경우에는 네임노드가 현재 실행되지 않고 있다는 것을 확신하기 어렵다. 고가용성을 구현하기 위해서는 기존의 활성 네임노드가 시스템을 손상시키거나 망가뜨리지 않도록 엄청난 노력을 기울여야하는데 이를 위해 **펜싱**이라는 메서드를 제공한다.

<br/>

## 명령행 인터페이스

```shell
hadoop fs -copyFromLocal input/docs/quangle.txt hdfs://localhost/user/tom/quangle.txt
hadoop fs -copyFromLocal input/docs/quangle.txt /user/tom/quangle.txt
# uri가 생략이 되면 core-site.xml에 설정된 기본값인 hdfs://localhost를 가져온다
hadoop fs -copyFromLocal input/docs/quangle.txt quangle.txt
# 절대 경로 대신 상대 경로를 사용하여 HDFS의 홈 디렉터리로 파일을 복사할 수 있다.
```

로컬 파일 quangle.txt 파일을 로컬호스트에서 실행되는 HDFS 인스턴스의 /user/tom/quangle.txt로 복사된다.  

```shell
hadoop fs -mkdir books
hadoop fs -ls .
```

<Br/>

## 하둡 파일시스템 

하둡은 파일시스템의 추상화 개념을 가지고 있고, HDFS는 그 구현체 중 하나일 뿐이다.  

### 인터페이스

하둡은 자바로 작성되었기 때문에 자바 API를 통해 하둡 파일시스템과 연동할 수 있다.  
예를 들어 파일시스템 쉘은 자바 FileSystem 클래스로, 파일시스템 연산을 제공하는 자바 애플리케이션이다.  

<Br/>

## 데이터 흐름

### 파일 읽기 상세

<img width="637" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/924b782b-6c83-43c3-b748-33e77079726f">

1. DistributedFileSystem 인스턴스인 FileSystem 객체의 open() 메서드를 호출하여 원하는 파일을 연다.

2. DistributedFileSystem은 파일의 첫 번째 블록 위치를 파악하기 위해 RPC를 사용하여 네임노드를 호출한다.

3. DistributedFileSystem은 클라이언트가 데이터를 읽을 수 있도록 FSDataInputStream을 반환한다.

   FSDataInputStream은 데이터노드와 네임노드의 I/O를 관리하는 DFSInputStream을 래핑하고있다.

   클라이언트는 스트림을 읽기 위해 read() 메서드를 호출한다.

4. 파일의 첫 번째 블록의 데이터노드 주소를 저장하고 있는 DFSInputStream은 가장 가까운 데이터노드와 연결하고 해당 스트림에 대해 read() 메서드를 반복적으로 호출하여 클라이언트로 모든 데이터를 전송한다.

5. 블록의 끝에 도달하면 Stream은 데이터노드의 연결을 닫고 다음 블록의 데이터노드를 찾는다.

6. 모든 블록에 대한 읽기가 끝나면 클라이언트는 FSDataInputStream의 close() 메서드를 호출한다.

데이터를 읽는 도중 통신 장애 발생 시 DFSInputStream은 해당 블록을 저장하고 있는 다른 데이터노드와 연결을 시도한다.  
또한 이후 블록에 대한 불필요한 재시도를 방지하기 위해 장애가 발생한 데이터노드를 기억해둔다. DFSInputStream은 데이터노드로부터 전송된 데이터의 체크섬도 검증한다. 블록이 손상되었으면 DFSInputStream은 다른 데이터노드에 있는 블록의 복제본을 읽으려 시도한다. 물론 손상된 블록에 대한 정보는 네임노드에 보고된다.  

<br/>

### 파일 쓰기 상세

<img width="651" alt="image" src="https://github.com/Be-poz/TIL/assets/45073750/affc9847-cf39-4ac4-a6c8-0dead23f1851">

1. DistributedFileSystem의 create() 를 호출하여 파일을 생성한다.

2. 새로운 파일을 생성하기 위해 네임노드에 RPC 요청을 보낸다.

3. 클라이언트가 데이터를 쓸 때 DFSOutputStream은 데이터를 패킷으로 분리하고, **데이터 큐**라 불리는 내부 큐로 패킷을 보낸다. DataStreamer는 데이터 큐에 있는 패킷을 처리한다. 먼저 네임노드에 복제본을 저장할 데이터노드의 목록을 요청한다. 데이터노드 목록에 포함된 노드는 파이프라인을 형성하는데, 복제 수준이 3이면 3개의 노드가 파이프라인에 속하게 된다. 

4. DataStreamer는 파이프라인의 첫 번째 데이터 노드로 패킷을 전송한다. 첫 번째 데이터노드는 각 패킷을 저장하고 그것을 파이프라인의 두 번째 데이터노드로 보낸다. 두 번째 노드는 이어서 세 번째 노드로 전달한다.

5. DfsOutputStream은 데이터노드의 승인 여부를 기다리는 **ack 큐**라 불리는 내부 패킷 큐를 유지한다. ack 큐에 있는 패킷은 파이프라인의 모든 데이터노드로부터 ack 응답을 받아야 제거된다.

   데이터노드에 장애가 발생하면 파이프라인이 닫히고 ack 큐에 있는 모든 패킷은 데이터 큐 앞쪽에 다시 추가된다. 정상 데이터노드는 네임노드로부터 새로운 ID를 다시 받는다. 장애가 발생한 데이터노드가 나중에 다시 복구되면 불완전한 블록은 삭제된다. 장애 데이터노드는 파이프라인에서 제거되고, 정상인 나머지 두 데이터노드로 새로운 파이프라인을 구성한다. 블록의 남은 데이터는 파이프라인의 정상 데이터노드로 전송된다. 네임노드는 해당 블록이 불완전 복제라는 것을 인식하고 있으므로 나중에 다른 노드에 복제본이 생성되도록 조치한다. 

6. 데이터 쓰기를 완료할 때 클라이언트는 스트림에 close() 메서드를 호출한다.

7. close 메서드는 데이터노드 파이프라인에 남아 있는 모든 패킷을 flush 하고 승인이 나기를 기다린다. 모든 패킷이 완전히 전송되면 네임노드에 '파일 완료' 신호를 보낸다.

<br/>

##  distcp로 병렬 복사하기

하둡은 병렬로 다량의 데이터를 하둡 파일시스템으로부터 복사하기 위한 distcp라는 유용한 프로그램을 제공한다.  

```shell
hadoop distcp file file2

hadoop distcp dir1 dir2

hadoop distcp -update dir1 dir2
```

1. 하나의 파일을 다른 파일로 복사

2. 디렉터리 복사, dir2가 존재하지 않는다면 새롭게 생성되고 dir1 디렉터리 안의 파일들이 복사된다.

   만약 dir2가 이미 존재한다면 dir2/dir1 형태로 dir1은 dir2 하위에 복사된다. -overwrite 옵션도 있다.

3. -update 옵션을 통해 변경이 이루어진 파일들만 복사할 수도 있다.



distcp는 맵리듀스 잡으로 구현되어 있고 클러스터 전반에 걸쳐 병렬로 수행되는 맵 태스크를 이용하여 복사 작업을 한다. 여기에 리듀서는 없다. 각 파일은 단일 맵으로 복사되고 대체로 동일 할당으로 파일을 버켓팅함으로써 distcp는 거의 같은 양의 데이터를 각 맵에 제공하려 한다. 기본값으로 최대 20개의 맵이 사용되며, distcp의 -m 옵션을 통해 변경할 수 있다.  

가장 일반적인 distcp 사용예로 두 HDFS 클러스터 간의 데이터 이동을 들 수 있다.  

```shell
hadoop distcp -update -delete -p hdfs://namenode1/foo hdfs://namenode2/foo
```

-delete 옵션은 원본 경로에는 존재하지 않고 타깃 경로에만 존재하는 파일들을 지우도록 하는 옵션이다.  
-p는 파일의 권한, 블록 사이즈 등의 파일 속성 정보를 복제 시 보전하려는 경우에 사용된다.  

<br/>

### HDFS 클러스터 균형 유지

데이터를 HDFS로 복사할 때는 클러스터의 균형을 고려하는 것이 중요하다. HDFS는 클러스터 전반에 걸쳐 파일 블록이 고르게 분산되었을 때 가장 잘 동작한다. 따라서 distcp가 이러한 클러스터의 균형 유지를 방해하지 않도록 보장해야 한다. 예를 들어 -m 옵션 값으로 1을 지정하면 단일 맵이 복사를 수행하는데, 이는 느리고 클러스터 자원을 효율적으로 사용하지 못하는 문제를 떠나 복사 과정은 각 블록의 첫 번째 복제본이 단일 맵을 실행하는 노드에 지정되어야 한다. 두 번째와 세 번째 복제본은 클러스터를 넘나들며 분산되겠지만 첫 번째 복사본이 있는 노드는 불균형이 될 것이다. 이 문제는 클러스터에 노드보다 더 많은 맵을 할당함으로써 피할 수 있다. 이러한 이유로 distcp를 기본적으로 노드당 20개의 맵과 함께 수행하는 것이 가장 좋은 출발점이다.  

그러나 클러스터가 불균형에서 벗어나는 것이 항상 가능하지는 않기 때문에 클러스터 전반에 걸쳐 블록 분산을 지속적으로 향상시키기 위한  balancer 도구를 사용할 수 있다.  

---

