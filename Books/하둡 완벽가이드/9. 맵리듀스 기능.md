# 맵리듀스 기능

## 카운터

카운터는 잡에 대한 통계 정보를 수집하는 유용한 채널로, 데이터 품질의 통제나 애플리케이션 수준의 통계를 제공한다.  
맵이나 리듀스 태스크에 로그 메시지를 넣어서 특정 조건의 발생을 기록하는 것보다는 카운터를 사용하는 것이 더 좋다.  

### 내장 카운터

내장 카운터 그룹

1. org.apache.hadoop.mapreduce.TaskCounter
2. org.apache.hadoop.mapreduce.FileSystemCounter
3. org.apache.hadoop.mapreduce.lib.input.FileInputFormatCounter
4. org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter
5. org.apache.hadoop.mapreduce.JobCounter



#### 태스크 카운터

태스크 카운터는 각 태스크가 실행될 때 해당 태스크에 대한 정보를 수집한 후 잡의 모든 태스크에 대한 값을 취합하여 최종 결과를 알려준다.  

태스크 카운터는 마지막 전송 이후에 변경된 수치만 보내는 것이 아니라 보낼 때마다 누적된 전체 수치를 전송하므로 메시지 유실로 인한 오류를 방지할 수 있다. 또한 잡 실행 도중 태스크가 실패하면 카운터는 중단된다. 

내장 맵리듀스 태스크 카운터

* 맵 입력 카운터
* 스플릿 원시 바이트
* 맵 출력 레코드
* 리듀스 입력 레코드

등등...  

내장 파일시스템 태스크 카운터

* 파일시스템에서 읽은 바이트
* 파일시스템에 쓴 바이트

등등...

#### 잡 카운터

잡 카운터는 태스크 수행 중에 변경되는 값이 아닌 잡 수준의 통계값을 측정한다.  

내장 잡 카운터

* 실행된 맵/리듀스/우버 태스크
* 실패한 맵/리듀스/우버 태스크
* 강제 종료된 맵/리듀스 태스크
* 맵/리듀스 태스크 전체 시간
* 등등...

<br/>

### 사용자 정의 자바 카운터

맵리듀스는 사용자 코드 수준에서 카운터 집합을 정의하게 해주며, 매퍼와 리듀서에서 원하는 방식으로 증가시킬 수 있다.  

```java
public class MaxTemperatureWithCounters extends Configured implements Tool {
  
  enum Temperature {
    MISSING,
    MALFORMED
  }
  
  static class MaxTemperatureMapperWithCounters
    extends Mapper<LongWritable, Text, Text, IntWritable> {
    
    private NcdcRecordParser parser = new NcdcRecordParser();
  
    @Override
    protected void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {
      
      parser.parse(value);
      if (parser.isValidTemperature()) {
        int airTemperature = parser.getAirTemperature();
        context.write(new Text(parser.getYear()),
            new IntWritable(airTemperature));
      } else if (parser.isMalformedTemperature()) {
        System.err.println("Ignoring possibly corrupt input: " + value);
        context.getCounter(Temperature.MALFORMED).increment(1);
      } else if (parser.isMissingTemperature()) {
        context.getCounter(Temperature.MISSING).increment(1);
      }
      
      // dynamic counter
      context.getCounter("TemperatureQuality", parser.getQuality()).increment(1);
    }
  }
  
  @Override
  public int run(String[] args) throws Exception {
    Job job = JobBuilder.parseInputAndOutput(this, getConf(), args);
    if (job == null) {
      return -1;
    }
    
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    job.setMapperClass(MaxTemperatureMapperWithCounters.class);
    job.setCombinerClass(MaxTemperatureReducer.class);
    job.setReducerClass(MaxTemperatureReducer.class);

    return job.waitForCompletion(true) ? 0 : 1;
  }
  
  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new MaxTemperatureWithCounters(), args);
    System.exit(exitCode);
  }
}
```

카운터는 연관된 카운터를 묶을 수 있도록 자바 enum으로 정의한다. 잡은 임의 개수의 enum을 정의할 수 있으며, 각  enum은 임의 개수의 필드를 갖는다. enum의 이름은 그룹명이고 enum의 필드는 카운터명이다.  

동적 카운터를 사용할 수도 있다.  

``public Counter getCounter(String groupName, String counterName)``   

```java
public class MissingTemperatureFields extends Configured implements Tool {

  @Override
  public int run(String[] args) throws Exception {
    if (args.length != 1) {
      JobBuilder.printUsage(this, "<job ID>");
      return -1;
    }
    String jobID = args[0];
    Cluster cluster = new Cluster(getConf());
    Job job = cluster.getJob(JobID.forName(jobID));
    if (job == null) {
      System.err.printf("No job with ID %s found.\n", jobID);
      return -1;
    }
    if (!job.isComplete()) {
      System.err.printf("Job %s is not complete.\n", jobID);
      return -1;
    }

    Counters counters = job.getCounters();
    long missing = counters.findCounter(
        MaxTemperatureWithCounters.Temperature.MISSING).getValue();
    long total = counters.findCounter(TaskCounter.MAP_INPUT_RECORDS).getValue();

    System.out.printf("Records with missing temperature fields: %.2f%%\n",
        100.0 * missing / total);
    return 0;
  }
  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new MissingTemperatureFields(), args);
    System.exit(exitCode);
  }
}
```

위와 같은 방법으로 카운터를 찾아서 값을 얻을 수 있다.  

<br/>

## 정렬

데이터를 정렬하는 기능은 맵리듀스의 핵심이다.  
애플리케이션 자체가 정렬과 굳이 관련이 없더라도 데이터를 정리하기 위해 맵리듀스가 제공하는 정렬 과정을 사용할 수 있다.  

```java
public class SortByTemperatureUsingHashPartitioner extends Configured
  implements Tool {
  
  @Override
  public int run(String[] args) throws Exception {
    Job job = JobBuilder.parseInputAndOutput(this, getConf(), args);
    if (job == null) {
      return -1;
    }
    
    job.setInputFormatClass(SequenceFileInputFormat.class);
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputFormatClass(SequenceFileOutputFormat.class);
    SequenceFileOutputFormat.setCompressOutput(job, true);
    SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
    SequenceFileOutputFormat.setOutputCompressionType(job,
        CompressionType.BLOCK);
    
    return job.waitForCompletion(true) ? 0 : 1;
  }
  
  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new SortByTemperatureUsingHashPartitioner(),
        args);
    System.exit(exitCode);
  }
}
```

맵리듀스는 기본적으로 키를 기준으로 입력 레코드를 정렬한다. IntWritable 키로 시퀀스 파일을 정렬하는 변형된 맵리듀스 잡이다. 히지만 출력물이 여러개 나왔을 때 전체적으로 정렬된 하나의 파일을 생성하기 위해 각 파일을 병합하는 것은 쉽지 않다.  

### 전체 정렬

단일 파티션을 사용하면 전체적으로 정렬된 파일을 생성하기가 간단하지만 단일 머신에서 모든 출력을 처리해야 하므로 좋지 않다.  

출력의 순서를 고려한 특정 파티셔너를 사용하면 된다. 기온 별로 파티션을 만들 수 있을 것이다. 하지만 특정 기온에 몰려있다면 파티션의 크기가 균등하지 않아진다. 이를 위해 **샘플링**을 통해 키의 범위를 파악할 수 있다.  

샘플링의 기본적인 아이디어는 키 분포를 짐작하기 위해 작은 크기의 키 집합을 살펴본 후 파티션 생성에 활용하는 것이다.  

```java
public class SortByTemperatureUsingTotalOrderPartitioner extends Configured
  implements Tool {
  
  @Override
  public int run(String[] args) throws Exception {
    Job job = JobBuilder.parseInputAndOutput(this, getConf(), args);
    if (job == null) {
      return -1;
    }
    
    job.setInputFormatClass(SequenceFileInputFormat.class);
    job.setOutputKeyClass(IntWritable.class);
    job.setOutputFormatClass(SequenceFileOutputFormat.class);
    SequenceFileOutputFormat.setCompressOutput(job, true);
    SequenceFileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
    SequenceFileOutputFormat.setOutputCompressionType(job,
        CompressionType.BLOCK);

    job.setPartitionerClass(TotalOrderPartitioner.class);

    InputSampler.Sampler<IntWritable, Text> sampler =
      new InputSampler.RandomSampler<IntWritable, Text>(0.1, 10000, 10);
    
    InputSampler.writePartitionFile(job, sampler);

    // Add to DistributedCache
    Configuration conf = job.getConfiguration();
    String partitionFile = TotalOrderPartitioner.getPartitionFile(conf);
    URI partitionUri = new URI(partitionFile);
    job.addCacheFile(partitionUri);

    return job.waitForCompletion(true) ? 0 : 1;
  }
  
  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(
        new SortByTemperatureUsingTotalOrderPartitioner(), args);
    System.exit(exitCode);
  }
}
```

``TotalOrderPartitioner``는 해시 값으로 파티션을 나누는 default로 동작하는  ``HashPartitioner``와 다르게 키의 정렬순서에 맞게끔 파티셔닝을 하게한다. 샘플러를 이용해 전체 데이터의 0.1 만큼 가져와 최대 10000개의 데이터를 파티션 10개에 나눠지게끔 샘플링하는 코드다. 샘플러는 ``TotalOrderPartitioner``에 필요한 파티션 파일을 작성한다고 보면 된다.  

### 2차 정렬

맵리듀스 프레임워크는 리듀서로 보내기 전에 키를 기준으로 레코드를 정렬한다. 하지만 특정 키에 대한 값은 정렬되지 않는다. 여러 맵 태스크에서 값이 전달되고 각 태스크의 종료 시점은 실행할 때마다 다르기 때문에 값이 나타나는 순서는 일정하지 않다.  

(연도, 기온) 의 키에서 연도와 기온을 내림차순으로 정렬된다면 첫 번째 값만 가져오면 그 값이 해당 연도의 최고 기온이라고 보면 될 것이다.  

키의 연도 부분을 기준으로 분할되도록 파티셔너를 설정하면 동일한 연도의 레코드는 동일한 리듀서로 가도록 보장할 수 있지만 리듀서가 파티션 내에서 키를 기준으로 레코드를 그룹화한다는 사실은 변경하지 않는다.  

이 그룹화를 제어해야 한다. 값으로 정렬한 효과를 얻는 방법은 다음과 같다.  

* 원래 키와 원래 값의 조합으로 새로운 키를 만든다.
* 정렬 비교자는 조합 키(즉, 원래 키와 원래 값)를 기준으로 정렬화한다.
* 조합 키에 대한 파티셔너와 그룹화 비교자는 파티셔닝과 그룹화를 수행할 때 원래 키만 고려한다.

```java
public class MaxTemperatureUsingSecondarySort
  extends Configured implements Tool {
  
  static class MaxTemperatureMapper
    extends Mapper<LongWritable, Text, IntPair, NullWritable> {
  
    private NcdcRecordParser parser = new NcdcRecordParser();
    
    @Override
    protected void map(LongWritable key, Text value,
        Context context) throws IOException, InterruptedException {
      
      parser.parse(value);
      if (parser.isValidTemperature()) {
        /*[*/context.write(new IntPair(parser.getYearInt(),
            parser.getAirTemperature()), NullWritable.get());/*]*/
      }
    }
  }
  
  static class MaxTemperatureReducer
    extends Reducer<IntPair, NullWritable, IntPair, NullWritable> {
  
    @Override
    protected void reduce(IntPair key, Iterable<NullWritable> values,
        Context context) throws IOException, InterruptedException {
      
      /*[*/context.write(key, NullWritable.get());/*]*/
    }
  }
  
  public static class FirstPartitioner
    extends Partitioner<IntPair, NullWritable> {

    @Override
    public int getPartition(IntPair key, NullWritable value, int numPartitions) {
      // multiply by 127 to perform some mixing
      return Math.abs(key.getFirst() * 127) % numPartitions;
    }
  }
  
  public static class KeyComparator extends WritableComparator {
    protected KeyComparator() {
      super(IntPair.class, true);
    }
    @Override
    public int compare(WritableComparable w1, WritableComparable w2) {
      IntPair ip1 = (IntPair) w1;
      IntPair ip2 = (IntPair) w2;
      int cmp = IntPair.compare(ip1.getFirst(), ip2.getFirst());
      if (cmp != 0) {
        return cmp;
      }
      return -IntPair.compare(ip1.getSecond(), ip2.getSecond()); //reverse
    }
  }
  
  public static class GroupComparator extends WritableComparator {
    protected GroupComparator() {
      super(IntPair.class, true);
    }
    @Override
    public int compare(WritableComparable w1, WritableComparable w2) {
      IntPair ip1 = (IntPair) w1;
      IntPair ip2 = (IntPair) w2;
      return IntPair.compare(ip1.getFirst(), ip2.getFirst());
    }
  }

  @Override
  public int run(String[] args) throws Exception {
    Job job = JobBuilder.parseInputAndOutput(this, getConf(), args);
    if (job == null) {
      return -1;
    }
    
    job.setMapperClass(MaxTemperatureMapper.class);
    /*[*/job.setPartitionerClass(FirstPartitioner.class);/*]*/
    /*[*/job.setSortComparatorClass(KeyComparator.class);/*]*/
    /*[*/job.setGroupingComparatorClass(GroupComparator.class);/*]*/
    job.setReducerClass(MaxTemperatureReducer.class);
    job.setOutputKeyClass(IntPair.class);
    job.setOutputValueClass(NullWritable.class);
    
    return job.waitForCompletion(true) ? 0 : 1;
  }
  
  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new MaxTemperatureUsingSecondarySort(), args);
    System.exit(exitCode);
  }
}
```

매퍼에서 연도와 기온을 조합한 키를 생성하고, 리듀서는 키에서 첫 번째 기온을 얻을 수 있고(최고 기온) 따라서 어떤 값도 필요 없기 때문에 에  Nullwritable을 사용하여 값을 생성했다. ``KeyComparator`` 를 이용해 정렬 기준도 정해주었다.  

이렇게 되면 손쉽게 해당 연도의 기온을 받아올 수가 있다.  

<br/>

## 조인

맵리듀스는 대용량 데이터셋 간의 조인을 수행할 수 있지만, 조인 코드를 밑바닥부터 작성하는 것은 매우 어려운 일이다.  
맵리듀스 프로그램을 직접 작성하는 대신 피그, 하이브, 캐스케이딩, 크럭, 스파크와 같은 고차원 프레임워크를 활용하는 것이 더 좋으며, 이들은 조인연산을 구현체의 핵심 부분으로 제공하고 있다.  

매퍼에 의해 조인이 수행되면 **맵-사이드 조인**이라 부르고, 리듀서에 의해 수행되면 **리듀스-사이드 조인**이라고 부른다.  

### 맵-사이드 조인

대용량 입력에 대한 맵-사이드 조인은 데이터가 맵 함수에 도달하기 전에 조인이 수행된다.  
이와 같이 작동하려면 각 맵의 입력이 특별한 방식으로 분할되고 정렬되어야 한다. 각 입력 데이터 셋은 반드시 동일한 개수의 파티션으로 분할되어야 하며, 각 원본은 동일한 조인키로 정렬되어 있어야 한다. 다시 말해, 특정한 키에 대한 모든 레코드는 동일한 파티션에 존재해야 한다.  

따라서 맵-사이드 조인은 동일한 개수의 리듀서, 동일한 키, 분리되지 않는 출력 파일을 가진 여러 잡의 출력을 조인하는 데 사용할 수 있다.  

### 리듀스-사이드 조인

입력 데이터셋을 일부러 특별한 방식으로 구조화할 필요가 없기 때문에 맵-사이드 조인보다 더 일반적이다.  
하지만 두 데이터셋 모두 맵리듀스의 셔플 단계를 거쳐야 한다는 비효율적인 면이 있다.  

기본적인 아이디어는 매퍼가 소스에 따라 각 레코드에 태그를 붙이고 조인키를 맵 출력키로 사용함으로써 동일한 키를 가진 레코드는 같은 리듀서에 함께 모이게 된다는 것이다.  

실무에서 리듀스-사이드 조인을 수행하려면 다음에 나오는 몇 개의 요소를 알아야 한다.

* 다중 입력
  * 일반적으로 데이터셋의 입력 원본은 서로 다른 포맷이므로  MultipleInputs 클래스를 사용하여 각 입력 원본을 분석하고 태깅하는 코드를 별도로 작성하는 것이 더 편하다.
* 2차 정렬
  * 조인을 수행하려면 한 원본의 데이터를 다른 원본보다 먼저 처리하는 것이 중요하다. 기상데이터 조인에서 기상관측소 레코드는 각 키의 값 중에서 앞부분에 있어야 한다. 그러면 리듀서는 뒤에 나오는 기상 레코드에 기상관측소의 이름을 넣어 함께 출력할 수 있따.  따라서 2차 정렬의 각 키의 값에 대해 순서를 보장하는 방법을 이용해야 한다.

```java
public class JoinStationMapper
    extends Mapper<LongWritable, Text, TextPair, Text> {
  private NcdcStationMetadataParser parser = new NcdcStationMetadataParser();

  @Override
  protected void map(LongWritable key, Text value, Context context)
      throws IOException, InterruptedException {
    if (parser.parse(value)) {
      context.write(new TextPair(parser.getStationId(), "0"),
          new Text(parser.getStationName()));
    }
  }
}

public class JoinRecordMapper
    extends Mapper<LongWritable, Text, TextPair, Text> {
  private NcdcRecordParser parser = new NcdcRecordParser();
  
  @Override
  protected void map(LongWritable key, Text value, Context context)
      throws IOException, InterruptedException {
    parser.parse(value);
    context.write(new TextPair(parser.getStationId(), "1"), value);
  }

}

public class JoinReducer extends Reducer<TextPair, Text, Text, Text> {

  @Override
  protected void reduce(TextPair key, Iterable<Text> values, Context context)
      throws IOException, InterruptedException {
    Iterator<Text> iter = values.iterator();
    Text stationName = new Text(iter.next());
    while (iter.hasNext()) {
      Text record = iter.next();
      Text outValue = new Text(stationName.toString() + "\t" + record.toString());
      context.write(key.getFirst(), outValue);
    }
  }
}

public class JoinRecordWithStationName extends Configured implements Tool {
  
  public static class KeyPartitioner extends Partitioner<TextPair, Text> {
    @Override
    public int getPartition(/*[*/TextPair key/*]*/, Text value, int numPartitions) {
      return (/*[*/key.getFirst().hashCode()/*]*/ & Integer.MAX_VALUE) % numPartitions;
    }
  }
  
  @Override
  public int run(String[] args) throws Exception {
    if (args.length != 3) {
      JobBuilder.printUsage(this, "<ncdc input> <station input> <output>");
      return -1;
    }
    
    Job job = new Job(getConf(), "Join weather records with station names");
    job.setJarByClass(getClass());
    
    Path ncdcInputPath = new Path(args[0]);
    Path stationInputPath = new Path(args[1]);
    Path outputPath = new Path(args[2]);
    
    MultipleInputs.addInputPath(job, ncdcInputPath,
        TextInputFormat.class, JoinRecordMapper.class);
    MultipleInputs.addInputPath(job, stationInputPath,
        TextInputFormat.class, JoinStationMapper.class);
    FileOutputFormat.setOutputPath(job, outputPath);
    
    /*[*/job.setPartitionerClass(KeyPartitioner.class);
    job.setGroupingComparatorClass(TextPair.FirstComparator.class);/*]*/
    
    job.setMapOutputKeyClass(TextPair.class);
    
    job.setReducerClass(JoinReducer.class);

    job.setOutputKeyClass(Text.class);
    
    return job.waitForCompletion(true) ? 0 : 1;
  }
  
  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new JoinRecordWithStationName(), args);
    System.exit(exitCode);
  }
}
```

TextPair를 통해 동일한 키에 대해서 어떤  value가 어떤 데이터에서 온 것인지 식별할 수 있게끔 했다.  

---

